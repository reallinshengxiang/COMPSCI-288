{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 4: Finetuning and Prompting\n\nIn this project, you will first learn how to use Huggingface's Transformers library to load large language models. Next, we will generate text from these models. Finally, we will work with a small text-to-SQL dataset, where the input is a natural language query and the output is a SQL query that can be executed against a database. You will explore (1) finetuning a pretrained language model, and (2) prompting the pretrained language model with examples. \n\nThis project will be more open ended than the previous projects. We expect you to learn how to use the huggingface and torch documentation.","metadata":{}},{"cell_type":"markdown","source":"## Setup\n\nFirst we install and import the required dependencies. These include:\n* `torch` for modeling and training\n* `transformers` for pre-trained models\n* `datasets` from huggingface to load existing datasets.","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install transformers\n!pip install datasets\n!pip install sentence-transformers\n\n# Standard library imports\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:12:23.232948Z","iopub.execute_input":"2025-01-14T15:12:23.233180Z","iopub.status.idle":"2025-01-14T15:12:47.509612Z","shell.execute_reply.started":"2025-01-14T15:12:23.233142Z","shell.execute_reply":"2025-01-14T15:12:47.508828Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU.\nWe'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging.","metadata":{}},{"cell_type":"code","source":"assert torch.cuda.is_available()\ndevice = torch.device(\"cuda\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:13:02.208113Z","iopub.execute_input":"2025-01-14T15:13:02.208434Z","iopub.status.idle":"2025-01-14T15:13:02.278100Z","shell.execute_reply.started":"2025-01-14T15:13:02.208409Z","shell.execute_reply":"2025-01-14T15:13:02.277157Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Loading Model","metadata":{}},{"cell_type":"markdown","source":"We will use GPT-2 Medium for this project. This includes both the GPT-2 tokenizer and the GPT-2 model weights itself. If you want to learn more about this model, you can read the GPT-2 paper https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\n\nLet's first load the tokenizer for the GPT-2 medium model. You can find how to do this by reading the documentation for AutoTokenzier in transformers, and finding the GPT-2 model of ~345 million params in there.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# Your code here\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n\ntokenizer.pad_token = tokenizer.eos_token # convenient for padding later","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:13:14.443120Z","iopub.execute_input":"2025-01-14T15:13:14.443458Z","iopub.status.idle":"2025-01-14T15:13:15.437236Z","shell.execute_reply.started":"2025-01-14T15:13:14.443429Z","shell.execute_reply":"2025-01-14T15:13:15.436144Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b5e6673d974af8bcae0fc96eb4e944"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef32be75d18478f940cd40dda25e357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e382b4ae7a43eebd93a75ac566a243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"180e773a5abd40ada5de319ca85153eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5259f313ad9f4563ba966ceacf5792ce"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Let's tokenize and detokenize some text from this model.","metadata":{}},{"cell_type":"code","source":"print(tokenizer.encode('Hello world'))\nprint(tokenizer.decode(tokenizer.encode('Hello world')))\nprint(tokenizer.encode(\"Hola, c√≥mo est√°süòç\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:14:21.924039Z","iopub.execute_input":"2025-01-14T15:14:21.924359Z","iopub.status.idle":"2025-01-14T15:14:21.932154Z","shell.execute_reply.started":"2025-01-14T15:14:21.924334Z","shell.execute_reply":"2025-01-14T15:14:21.931455Z"}},"outputs":[{"name":"stdout","text":"[15496, 995]\nHello world\n[39, 5708, 11, 269, 10205, 5908, 1556, 40138, 47249, 235]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Now let's load the GPT-2 Medium model. Make sure you also put the model onto the GPU.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n# Your code here\ngpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:54:05.898916Z","iopub.execute_input":"2025-01-14T15:54:05.899200Z","iopub.status.idle":"2025-01-14T15:54:06.657189Z","shell.execute_reply.started":"2025-01-14T15:54:05.899178Z","shell.execute_reply":"2025-01-14T15:54:06.656509Z"}},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":"## Generate From the Model","metadata":{}},{"cell_type":"markdown","source":"Now let's generate some text from the model to test its LM capabilities. Let's first generate one output of length 50 tokens using greedy decoding (temperature = 0), which should get us some text with high likelihood under the model. When generating text, you can condition on phrases such as \"The coolest thing in NLP right now is\". Find the relevant function and arguments to use for generating text using the Huggingface documentation.\n\nHint: you may find https://huggingface.co/docs/transformers/main_classes/text_generation to be useful for learning about generating from LMs.","metadata":{}},{"cell_type":"code","source":"inputs = tokenizer(\"The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n# Your code here\nsample_output = gpt2_model.generate(inputs, do_sample=True, temperature=0.7, top_k=2, max_new_tokens=50)[0]\n\nprint(\"{}\".format(tokenizer.decode(sample_output, skip_special_tokens=True)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:04.357223Z","iopub.execute_input":"2025-01-14T15:45:04.357585Z","iopub.status.idle":"2025-01-14T15:45:05.195111Z","shell.execute_reply.started":"2025-01-14T15:45:04.357558Z","shell.execute_reply":"2025-01-14T15:45:05.194183Z"}},"outputs":[{"name":"stdout","text":"The coolest thing right now in NLP is the ability to create a deep understanding of the human brain. We're not just talking about a few hundred words here, we're talking about a whole brain. We're not talking about a few hundred words, we're talking about a whole brain.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"Now let's generate 10 pieces of random text of length 50 tokens from the model using random sampling with temperature set to 0.7. This will allow the text to be somewhat higher in diversity (random sampling) while maintaining reasonable quality (temperature < 1). ","metadata":{}},{"cell_type":"code","source":"inputs = tokenizer(\"The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n# Your code here\nsample_outputs = gpt2_model.generate(inputs, num_return_sequences=10, do_sample=True, temperature=0.7, top_k=2, max_new_tokens=50)\n\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:08.236054Z","iopub.execute_input":"2025-01-14T15:45:08.236378Z","iopub.status.idle":"2025-01-14T15:45:09.129260Z","shell.execute_reply.started":"2025-01-14T15:45:08.236352Z","shell.execute_reply":"2025-01-14T15:45:09.128472Z"}},"outputs":[{"name":"stdout","text":"0: The coolest thing right now in NLP is the ability to create a machine learning model that can predict the future. It's not just about predicting the future, it's about predicting what's going to happen in the future.\n\nI'm a big believer in machine learning, but it's\n1: The coolest thing right now in NLP is the ability to use machine learning to predict the future.\n\nThe most interesting part of this is that we can predict what the future will be.\n\nWe know what the future is going to be, but we don't know what the future\n2: The coolest thing right now in NLP is that it's not just about the data. It's about the people who are doing it, and how they're doing it. It's about the people who are using it, and how they're using it.\n\nI'm a big believer\n3: The coolest thing right now in NLP is the ability to use machine learning to predict the future. We can use machine learning to predict the future of a company, a product, or a person. We have a lot more data than we have ever had before. We have a lot more data\n4: The coolest thing right now in NLP is the ability to use machine learning to predict the future.\n\nI'm not sure how much of this is a result of the fact that NLP has been around for a while, but it's certainly something that's become more popular.\n\n\n5: The coolest thing right now in NLP is the ability to use the data to create a model that can predict the behavior of a user.\n\nThe data is available in a variety of formats. You can download the data from the NLP website, download the data from the NLP API\n6: The coolest thing right now in NLP is the ability to create a deep understanding of the human brain. It's a very powerful tool, but we need to be able to use it in a way that is not only useful but also useful for the rest of the world.\"\n\nThe researchers\n7: The coolest thing right now in NLP is that it's not about the data. It's about the data itself. It's about the data that you can use to build a model of your own.\n\nThe best way to do that is to use a model.\n\nI've\n8: The coolest thing right now in NLP is the ability to use the same language in different ways. You can say \"I want to know what the user wants\" and then you can say \"I want to know what the user wants to know\" and then you can say \"I want to\n9: The coolest thing right now in NLP is the ability to use the data to create a machine learning model. This means that you can use the data to learn about your customers, and then apply machine learning to improve the product or service.\n\nThe data can be anything, from a simple\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"## Text-to-SQL Task Setup","metadata":{}},{"cell_type":"markdown","source":"First, let's download the data of text-to-SQL pairs and the database against which we'll execute queries to retrieve answers.\n\nThe code below initializes the database and does some initial preprocessing data preprocessing + splitting for you.","metadata":{}},{"cell_type":"code","source":"%%capture\n!wget https://github.com/jkkummerfeld/text2sql-data/raw/master/data/geography.json\n!wget https://github.com/jkkummerfeld/text2sql-data/raw/master/data/geography-db.sql\n\nimport re\nimport sqlite3\nimport json\nfrom copy import deepcopy\n\nDATABASE_NAME = 'geo.db'\nSQL_FILENAME = 'geography-db.sql'\nDATASET_FILENAME = 'geography.json'\n\nwith open(SQL_FILENAME, 'r') as file:\n    sql_script = file.read()\n    sql_script = re.sub(r\"\\s*ENGINE=[^ ]+\",\"\", sql_script)\n    sql_script = re.sub(r\"\\s*DEFAULT CHARSET=[^ ;]+\",\"\", sql_script)\n    sql_script = re.sub(r\"\\s*LOCK TABLES `[^`]+` WRITE;\",\"\", sql_script)  # remove LOCK TABLES\n    sql_script = re.sub(r\"\\s*UNLOCK TABLES;\",\"\", sql_script)  # remove UNLOCK TABLES\n    sql_script = sql_script.replace('`', '')  # remove backticks\n\n# Connect to the SQLite database (this will create the file if it doesn't exist)\nconnection = sqlite3.connect(DATABASE_NAME)\nprint(sql_script)\n\nconnection.executescript(sql_script)\nconnection.commit()\nconnection.close()\n\nconnection = sqlite3.connect(DATABASE_NAME)\ncursor = connection.cursor()\nwith open(DATASET_FILENAME, 'r') as file:\n    dataset = json.load(file)\n\nsplits = {'train': [], 'dev': [], 'test': []}\nfor query_type in dataset:\n    for example in query_type['sentences']:\n        split = example['question-split']\n        example['question'] = example['text']\n        for key, value in example['variables'].items():\n            example['question'] = example['question'].replace(key, value)\n        example['sql'] = deepcopy(query_type['sql'])\n        example['sql'] = example['sql'][0]\n        for key, value in example['variables'].items():\n            example['sql'] = example['sql'].replace(key, value)\n        try:\n            cursor.execute(example['sql'])\n        except:\n            continue\n        example['db_answer'] = cursor.fetchall()\n        del example['text']\n        del example['variables']\n        splits[split].append(example)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:13.536593Z","iopub.execute_input":"2025-01-14T15:45:13.536916Z","iopub.status.idle":"2025-01-14T15:45:14.850766Z","shell.execute_reply.started":"2025-01-14T15:45:13.536890Z","shell.execute_reply":"2025-01-14T15:45:14.849577Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"We also provide a function you can use to query the database:","metadata":{}},{"cell_type":"code","source":"def query_db(sql):\n    connection = sqlite3.connect(DATABASE_NAME)\n    cursor = connection.cursor()\n    cursor.execute(sql)\n    result = cursor.fetchall()\n    connection.close()\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:19.018944Z","iopub.execute_input":"2025-01-14T15:45:19.019247Z","iopub.status.idle":"2025-01-14T15:45:19.023856Z","shell.execute_reply.started":"2025-01-14T15:45:19.019225Z","shell.execute_reply":"2025-01-14T15:45:19.022819Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"This dataset is pretty small:","metadata":{}},{"cell_type":"code","source":"print('Train set size:', len(splits['train']))\nprint('Dev set size:', len(splits['dev']))\nprint('Test set size:', len(splits['test']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:24.372967Z","iopub.execute_input":"2025-01-14T15:45:24.373260Z","iopub.status.idle":"2025-01-14T15:45:24.379090Z","shell.execute_reply.started":"2025-01-14T15:45:24.373236Z","shell.execute_reply":"2025-01-14T15:45:24.378234Z"}},"outputs":[{"name":"stdout","text":"Train set size: 547\nDev set size: 48\nTest set size: 277\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"Let's inspect an example from the training dataset:","metadata":{}},{"cell_type":"code","source":"splits['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:27.822068Z","iopub.execute_input":"2025-01-14T15:45:27.822459Z","iopub.status.idle":"2025-01-14T15:45:27.829219Z","shell.execute_reply.started":"2025-01-14T15:45:27.822411Z","shell.execute_reply":"2025-01-14T15:45:27.828122Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"{'question-split': 'train',\n 'question': 'what is the biggest city in nebraska',\n 'sql': 'SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"nebraska\" ) AND CITYalias0.STATE_NAME = \"nebraska\" ;',\n 'db_answer': [('omaha',)]}"},"metadata":{}}],"execution_count":49},{"cell_type":"markdown","source":"Note that the `db_answer` is the result of executing the given SQL output against the database:","metadata":{}},{"cell_type":"code","source":"query_db(splits['train'][0]['sql'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:31.616916Z","iopub.execute_input":"2025-01-14T15:45:31.617208Z","iopub.status.idle":"2025-01-14T15:45:31.622786Z","shell.execute_reply.started":"2025-01-14T15:45:31.617187Z","shell.execute_reply":"2025-01-14T15:45:31.622084Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"[('omaha',)]"},"metadata":{}}],"execution_count":50},{"cell_type":"markdown","source":"Let's check how well our language model does on this text-to-SQL task out of the box. You can just use greedy decoding. ","metadata":{}},{"cell_type":"code","source":"prompt = \"Write a SQL query based on the following question.\\n\\nQuestion: {input}\\n\\nSQL:\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:36.172520Z","iopub.execute_input":"2025-01-14T15:45:36.172819Z","iopub.status.idle":"2025-01-14T15:45:36.176458Z","shell.execute_reply.started":"2025-01-14T15:45:36.172796Z","shell.execute_reply":"2025-01-14T15:45:36.175572Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Your code here. Generate from the model using greedy decoding with the above prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n#output = gpt2_model.generate(inputs, do_sample=True, temperature=0.7, top_k=2, max_new_tokens=50)[0]\noutput = gpt2_model.generate(inputs, max_length=50, temperature=0, num_return_sequences=1)[0]\npredicted_sql = tokenizer.decode(output, skip_special_tokens=True) \nprint(predicted_sql)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:39.898134Z","iopub.execute_input":"2025-01-14T15:45:39.898463Z","iopub.status.idle":"2025-01-14T15:45:40.356044Z","shell.execute_reply.started":"2025-01-14T15:45:39.898435Z","shell.execute_reply":"2025-01-14T15:45:40.355319Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Write a SQL query based on the following question.\n\nQuestion: {input}\n\nSQL: SELECT * FROM {input}\n\nOutput:\n\nSELECT * FROM {input}\n\nQuestion: {input}\n\nSQL:\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"You should get something that looks kind of like a SQL query, but it probably won't match the correct output, and in fact it most likely won't even execute without crashing when you try to query the database (you'll see a syntax error below).","metadata":{}},{"cell_type":"code","source":"try:\n    query_db(predicted_sql)\n    print('success!')\nexcept:\n    print('failed to execute!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:46.166695Z","iopub.execute_input":"2025-01-14T15:45:46.166975Z","iopub.status.idle":"2025-01-14T15:45:46.171764Z","shell.execute_reply.started":"2025-01-14T15:45:46.166953Z","shell.execute_reply":"2025-01-14T15:45:46.170961Z"}},"outputs":[{"name":"stdout","text":"failed to execute!\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"Let's confirm quantitatively that the model doesn't work well out-of-the-box by running on the dev dataset (`splits['dev']`).","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef predict_greedy(model, data, max_new_tokens=128):\n    \"\"\"\n    Return the model's greedy text-to-sql predictions on the given data split.\n    The maximum number of new tokens generated (NOT including tokens in the prompt) should be equal to max_new_tokens.\n    For speed, you should batch the generation. The tokenizer can handle multiple inputs simultaneously,\n    but you'll need to tell it to pad using padding=True, and you may also need to set tokenizer.padding_side='left'.\n    Hint: as a postprocessing step after you're done, you may need to cut off the output at the first appearance of '\\n' if the output is continuing past the end of the SQL.\n    \"\"\"\n    questions = [d['question'] for d in data]\n    predicted_sqls = []\n    # Your code here\n    batch_size=8\n    model=model.to(device)\n    model.eval()\n    tokenizer.padding_side='left'\n    PROMPT = \"Question: {question}\\n\\nSQL:\"\n    questions=[PROMPT.format(question=i) for i in questions]\n    for i in range(0, len(questions), batch_size):\n        batch_questions = questions[i:i + batch_size]\n        inputs = tokenizer(batch_questions, return_tensors=\"pt\", padding=True).input_ids.cuda()\n        output = model.generate(inputs, max_length=max_new_tokens, num_return_sequences=1, temperature=0,do_sample=False)\n        generated_sqls = tokenizer.batch_decode(output, skip_special_tokens=True, max_length=max_new_tokens)\n        \n        for i in range(len(generated_sqls)):\n            if '\\n\\n' in generated_sqls[i]:\n                components = generated_sqls[i].split('\\n\\n')\n                predicted_sqls.append(components[1].replace(\"SQL: \", \"\"))\n    return predicted_sqls # list of strings containing SQL predictions for each question in the data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:49.226776Z","iopub.execute_input":"2025-01-14T15:45:49.227085Z","iopub.status.idle":"2025-01-14T15:45:49.233647Z","shell.execute_reply.started":"2025-01-14T15:45:49.227061Z","shell.execute_reply":"2025-01-14T15:45:49.232861Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def check_execution_accuracy(predictions, data):\n    assert len(predictions) == len(data)\n    correct = 0\n    for p, d in zip(predictions, data):\n        try:\n            if query_db(p) == d['db_answer']:\n                correct += 1\n        except: # failed to execute\n            pass\n    return correct / len(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:53.048188Z","iopub.execute_input":"2025-01-14T15:45:53.048502Z","iopub.status.idle":"2025-01-14T15:45:53.052903Z","shell.execute_reply.started":"2025-01-14T15:45:53.048477Z","shell.execute_reply":"2025-01-14T15:45:53.051952Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"predictions = predict_greedy(gpt2_model, splits['dev'])\nprint('example prediction:', predictions[0])\nprint('initial execution acc', check_execution_accuracy(predictions, splits['dev']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:45:56.441299Z","iopub.execute_input":"2025-01-14T15:45:56.441600Z","iopub.status.idle":"2025-01-14T15:46:07.595581Z","shell.execute_reply.started":"2025-01-14T15:45:56.441575Z","shell.execute_reply":"2025-01-14T15:46:07.594816Z"}},"outputs":[{"name":"stdout","text":"example prediction: SQL:\ninitial execution acc 0.0\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"You will probably observe an accuracy around 0-2%. (It may be hard to verify if your `predict_greedy` function is correct at this stage, because the expected accuracy is so low, but you will reuse it later with an improved model, at which point it will be more obvious if your implementation is correct.)","metadata":{}},{"cell_type":"markdown","source":"## Model Finetuning","metadata":{}},{"cell_type":"markdown","source":"Now let's prepare our dataset for finetuning (i.e., training our pretrained language model on this text-to-SQL training set). For each element in the dataset, it should have a text prompt and then the SQL output, similar to above. Your job is to fill in the labels field below. This field sets the labels to use for training during the language modeling task.\n\nFor the labels, we only want to train the model to output the text after the word \"SQL:\". This is because in the prompt, everything before the word \"SQL:\" will also be provided to the model as input. Hint: use -100 as the label for tokens you do not want to train on. Hint 2: When doing LM training, the labels are the same as the input tokens, except shifted to the left by one. You should check whether Huggingface is already doing the shifting, or whether you need to do the shifting yourself.\n\nOne thing to be careful of with all LMs is to make sure there are not extra spaces. So, the text should be formatted as like \"SQL: {sql output}\" not \"SQL: {sql output} \". ","metadata":{}},{"cell_type":"code","source":"class Text2SQLDataset(Dataset):\n    PROMPT = \"Write a SQL query based on the following question.\\n\\nQuestion: {question}\\n\\nSQL: {sql}\"\n\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n        tokenizer.padding_side = 'right'\n\n        self.input_ids = []\n        self.attn_masks = []\n        self.labels = []\n\n        training_texts = []\n        for example in self.data:\n            training_text = Text2SQLDataset.PROMPT.format(question=example['question'], sql=example['sql']) + \"<|endoftext|>\" # include the end token so model knows when to stop!\n            training_texts.append(training_text)\n        encodings_dict = self.tokenizer(training_texts, padding=True, truncation=True)\n        for i, (example, training_text) in enumerate(zip(data, training_texts)):\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids'][i]))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask'][i]))\n            # Your code here\n            input_tokens = self.tokenizer.tokenize(training_text)\n            sql_start = input_tokens.index('SQL') \n            label = encodings_dict['input_ids'][i]\n            label[:sql_start]=[-100]*sql_start\n            self.labels.append(torch.tensor(label))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {'input_ids':(self.input_ids[idx]),\n                'attn_masks': (self.attn_masks[idx]),\n                'labels': (self.labels[idx])}\n        #return self.input_ids[idx], self.attn_masks[idx], self.labels[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:46:16.835854Z","iopub.execute_input":"2025-01-14T15:46:16.836145Z","iopub.status.idle":"2025-01-14T15:46:16.844197Z","shell.execute_reply.started":"2025-01-14T15:46:16.836122Z","shell.execute_reply":"2025-01-14T15:46:16.843185Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"train_dataset = Text2SQLDataset(splits['train'], tokenizer)\ndev_dataset = Text2SQLDataset(splits['dev'], tokenizer)\ntest_dataset = Text2SQLDataset(splits['test'], tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:46:20.273617Z","iopub.execute_input":"2025-01-14T15:46:20.273939Z","iopub.status.idle":"2025-01-14T15:46:20.729390Z","shell.execute_reply.started":"2025-01-14T15:46:20.273912Z","shell.execute_reply":"2025-01-14T15:46:20.728431Z"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"Now we can use the Huggingface Trainer to finetune GPT-2 Medium on this dataset. This abstracts away all of the details of training. Setup the training arguments to perform 3 epochs of training on this dataset, use a per-device batch size of 2 with gradient accumulation set to 8, use 30 warmup steps, a weight decay of 0.05. Set the eval batch size to be 8. Save a checkpoint after 100 steps. Set fp16 to True. Save the checkpoint in a specific output_dir so you can load it later. Hint: if it tries to launch Wandb, you may add the argument report_to=\"none\".","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, DataCollatorWithPadding\n\n# Define the output directory to save checkpoints\noutput_dir = \"/kaggle/working/checkpoint\"\n\n\n# Training arguments\n# ËÆæÁΩÆËÆ≠ÁªÉÂèÇÊï∞\ntraining_args = TrainingArguments(\n    output_dir=\"./output\",              # ‰øùÂ≠òÊ®°ÂûãÁöÑË∑ØÂæÑ\n    evaluation_strategy=\"steps\",        # ÊØèÈöîËã•Âπ≤Ê≠•ËøõË°åËØÑ‰º∞\n    num_train_epochs=3,                 # ËÆ≠ÁªÉ3‰∏™epoch\n    per_device_train_batch_size=2,      # ÊØè‰∏™ËÆæÂ§áÁöÑËÆ≠ÁªÉÊâπÊ¨°Â§ßÂ∞è‰∏∫2\n    gradient_accumulation_steps=8,      # Ê¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞‰∏∫8\n    warmup_steps=30,                    # 30‰∏™warmupÊ≠•Êï∞\n    weight_decay=0.05,                  # ÊùÉÈáçË°∞Âáè0.05\n    save_steps=100,                     # ÊØè100Ê≠•‰øùÂ≠ò‰∏Ä‰∏™Ê£ÄÊü•ÁÇπ\n    fp16=True,                          # ‰ΩøÁî®16‰ΩçÊµÆÁÇπÊï∞ËøõË°åËÆ≠ÁªÉ\n    report_to=\"none\",                   # Á¶ÅÁî®WandbÊä•Âëä\n    logging_dir='./logs',               # Êó•Âøó‰øùÂ≠òÁõÆÂΩï\n    logging_steps=10                    # ÊØè10Ê≠•ËÆ∞ÂΩï‰∏ÄÊ¨°Êó•Âøó\n)\n\n# print(train_dataset.__getitem__(0))\ndata_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer=Trainer(\n    model=gpt2_model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n    eval_dataset=dev_dataset\n)\n\ntrainer.train()\n# model.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:54:19.127175Z","iopub.execute_input":"2025-01-14T15:54:19.127483Z","iopub.status.idle":"2025-01-14T16:00:09.419245Z","shell.execute_reply.started":"2025-01-14T15:54:19.127459Z","shell.execute_reply":"2025-01-14T16:00:09.418356Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [51/51 05:43, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.781600</td>\n      <td>0.539835</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.374400</td>\n      <td>0.106600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.080500</td>\n      <td>0.045324</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.044700</td>\n      <td>0.032861</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.033200</td>\n      <td>0.029417</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=51, training_loss=0.8463447819664782, metrics={'train_runtime': 349.5598, 'train_samples_per_second': 4.694, 'train_steps_per_second': 0.146, 'total_flos': 892894929469440.0, 'train_loss': 0.8463447819664782, 'epoch': 2.978102189781022})"},"metadata":{}}],"execution_count":64},{"cell_type":"markdown","source":"Reload the final saved version of the model below. You may need to delete the previously loaded model if you run out of GPU memory.","metadata":{}},{"cell_type":"code","source":"del gpt2_model\n# Your code here\nfinetuned_model = AutoModelForCausalLM.from_pretrained('/kaggle/working/checkpoint/checkpoint-51')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T16:00:18.200059Z","iopub.execute_input":"2025-01-14T16:00:18.200569Z","iopub.status.idle":"2025-01-14T16:00:18.349105Z","shell.execute_reply.started":"2025-01-14T16:00:18.200491Z","shell.execute_reply":"2025-01-14T16:00:18.348195Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"Let's check our finetuned model's performance.","metadata":{}},{"cell_type":"code","source":"finetuned_predictions = predict_greedy(finetuned_model, splits['test'])\nprint('finetuned execution acc:', check_execution_accuracy(finetuned_predictions, splits['test']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T16:00:21.985122Z","iopub.execute_input":"2025-01-14T16:00:21.985474Z","iopub.status.idle":"2025-01-14T16:01:08.710882Z","shell.execute_reply.started":"2025-01-14T16:00:21.985443Z","shell.execute_reply":"2025-01-14T16:01:08.710002Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"finetuned execution acc: 0.27075812274368233\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"You should achieve an accuracy of roughly 50% using the suggested training hyperparameters; we will check >40% in the autograder.","metadata":{}},{"cell_type":"markdown","source":"Save your predictions.","metadata":{}},{"cell_type":"code","source":"def save_predictions(predictions, filename):\n    with open(filename, 'w') as f:\n        f.write('\\n'.join(predictions))\n\nsave_predictions(finetuned_predictions, 'finetuned_predictions.txt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Inspect some of your predictions compared to the correct outputs, and describe some common types of errors in your report. What fraction of errors are due to failing to execute (e.g., syntax error), and what fraction are due to executing but getting the wrong answer?","metadata":{}},{"cell_type":"code","source":"# Your code here\nlen(finetuned_predictions),len(splits['test'])\ncnt=0\nfor i in range(len(finetuned_predictions)):\n    if finetuned_predictions[i]!=splits['test'][i]['sql']:\n        cnt+=1\n        print(i,finetuned_predictions[i],\"\\n\",splits['test'][i]['sql'])\nprint(cnt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also examine the exact match accuracy (i.e., requiring the predicted SQL string to exactly match the gold answer) rather than the execution accuracy (just checking whether the output of executing the SQL against the database is the same).","metadata":{}},{"cell_type":"code","source":"def check_exact_match_accuracy(predictions, data):\n    assert len(predictions) == len(data)\n    correct = 0\n    for p, d in zip(predictions, data):\n        if p == d['sql']:\n            correct += 1\n    return correct / len(predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('finetuned exact match acc:', check_exact_match_accuracy(finetuned_predictions, splits['test']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The exact match accuracy will likely be close to the execution accuracy, but not exactly the same. What are some potential pros and cons of each metric? Discuss in your report.","metadata":{}},{"cell_type":"markdown","source":"Unload your finetuned model so that you don't run out of GPU memory later.","metadata":{}},{"cell_type":"code","source":"del finetuned_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Few-Shot Prompting","metadata":{}},{"cell_type":"markdown","source":"For the final part of this project, you will explore few-shot prompting, i.e., simply prompting the pretrained language model out-of-the-box using a small number of examples rather than finetuning.\n\nFirst, let's try just selecting 4 examples completely at random from the training set. Rewrite your `predict_greedy` function to change the prompt:","metadata":{}},{"cell_type":"code","source":"import random\n\n\nfew_shot_prompt = \"Question: {question0}\\n\\nSQL: {sql0}\\n\\n\\n\\n\" + \\\n    \"Question: {question1}\\n\\nSQL: {sql1}\\n\\n\\n\\n\" + \\\n    \"Question: {question2}\\n\\nSQL: {sql2}\\n\\n\\n\\n\" + \\\n    \"Question: {question3}\\n\\nSQL: {sql3}\\n\\n\\n\\n\" + \\\n    \"Question: {question}\\n\\nSQL:\"\n\n\ndef select_random_examples(question, few_shot_data, num_examples=4):\n    \"\"\"\n    Return a list containing 4 of the elements of few_shot_data, selected randomly\n    \"\"\"\n    return random.sample(few_shot_data, num_examples)\n\n\n@torch.no_grad()\ndef predict_greedy_fewshot(model, data, few_shot_data, max_new_tokens=128, example_selection_method=select_random_examples):\n    \"\"\"\n    Return the model's greedy text-to-sql predictions on the given data split.\n    The maximum number of new tokens generated (NOT including tokens in the prompt) should be equal to max_new_tokens.\n    The four examples with their SQL outputs should go in {question1}, {sql1}, {question2}, {sql2}, etc. in the few_shot_prompt. \n    The final {question} is the question that we're currently evaluating on.\n    \"\"\"\n    questions = [d['question'] for d in data]\n    predicted_sqls = []\n    prompts = []\n    for question in questions:\n        few_shot_examples = example_selection_method(question, few_shot_data, num_examples=4)\n        prompts.append(few_shot_prompt.format(\n            question0=few_shot_examples[0]['question'],\n            sql0=few_shot_examples[0]['sql'],\n            question1=few_shot_examples[1]['question'],\n            sql1=few_shot_examples[1]['sql'],\n            question2=few_shot_examples[2]['question'],\n            sql2=few_shot_examples[2]['sql'],\n            question3=few_shot_examples[3]['question'],\n            sql3=few_shot_examples[3]['sql'],\n            question=question\n        ))\n    # Your code here; should be fairly similar to your previous predict_greedy code.\n    # Hint: if you batch, we recommend batch size 8-16.\n    batch_size=8\n    tokenizer.padding_side='left'\n    all_sqls=[]\n    for i in range(0, len(data)):\n        inputs = tokenizer(prompts[i], return_tensors=\"pt\", padding=True).input_ids.cuda()\n        output = model.generate(inputs, max_length=max_new_tokens, num_return_sequences=1, temperature=0,do_sample=False)\n        generated_sqls = tokenizer.batch_decode(output, skip_special_tokens=True, max_length=max_new_tokens)\n        generated_sqls_string=generated_sqls[0].split(\"\\n\")\n        sql_strings= [s for s in generated_sqls_string if s.startswith('SQL: ')]\n        all_sqls.append(sql_strings[4])\n\n    predicted_sqls = [sql.replace(\"SQL:\", \"\") for sql in all_sqls]\n    return predicted_sqls # list of strings containing SQL predictions for each question in the data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reload the gpt2 model if you need to\ngpt2_model = AutoModelForCausalLM.from_pretrained('gpt2-medium').to(device) # Your code here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This call can take a few minutes even if you batch; you can debug on a subset of the dev set as needed.\npredictions = predict_greedy_fewshot(gpt2_model, splits['test'], splits['train'])\nprint('4-shot prompting with random examples, execution acc:', check_execution_accuracy(predictions, splits['test']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You will probably observe between 0-5% accuracy. Random example selection doesn't work very well on this dataset. \n\nHowever, what if we select examples by picking the examples from the training set whose questions are most similar to our current question? To do this, load a pretrained sentence encoder, which takes a sentence as input and outputs a fixed-length vector encoding semantic information about that sentence. First compute the vectors associated with all the training set questions, and then select examples from the training set based on which question vectors have the largest dot products with the vector for your current question. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sentence_transformers import SentenceTransformer\n\n# Your code here; load the sentence encoder (see https://www.sbert.net/ for documentation). A good choice of model is \"all-MiniLM-L6-v2\"\nsentence_encoder = SentenceTransformer('all-MiniLM-L6-v2') \n\ndef compute_question_encodings(data):\n    \"\"\"\n    For each example in the data, add a field called 'question_encoding' to the example, which is the vector encoding of the question.\n    \"\"\"\n    # Your code here\n    vectors = sentence_encoder.encode([d['question'] for d in data],show_progress_bar=False)\n    for example, vector in zip(data, vectors):\n        example['question_encoding'] = np.array(vector)\n\n\ndef select_similar_examples(question, few_shot_data, num_examples=4):\n    \"\"\"\n    Return a list containing 4 of the elements of few_shot_data, selected with questions most semantically similar to the given question. \n    The most similar question should be the LAST element of the list, second most similar should be the second to last element, etc.\n    The reason is that in the few-shot prompt, **you want the best example to be the most recent one.**\n\n    To rank by semantic similarity, first compute the vector for the current question, then compute its dot product with\n    all training set vectors (hint: you may want to vectorize this computation using numpy). Then sort by dot product.\n\n    You should take advantage of the 'question_encoding' field that you added to each example in compute_question_encodings.\n    \"\"\"\n    # Your code here\n    # hint: when you call .encode with your sentence encoder, use show_progress_bar=False to avoid tons of printouts\n    current_question_encoding = sentence_encoder.encode([question], show_progress_bar=False)[0]\n    similarities = np.dot(current_question_encoding, np.array([example['question_encoding'] for example in few_shot_data]).T)\n    sorted_indices = np.argsort(similarities)[::-1]\n    selected_examples = [few_shot_data[i] for i in sorted_indices[:num_examples]]\n    \n    return selected_examples\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# first precompute all the vectors for the training set\ncompute_question_encodings(splits['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# this call will again take a few minutes, even if you batched; feel free to debug on smaller sets of dev\npredictions = predict_greedy_fewshot(gpt2_model, splits['test'], splits['train'], example_selection_method=select_similar_examples)\nprint('4-shot prompting with similar examples, execution acc:', check_execution_accuracy(predictions, splits['test']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You should now achieve about 34% accuracy. The autograder will check that you get >30%.","metadata":{}},{"cell_type":"markdown","source":"Save your predictions.","metadata":{}},{"cell_type":"code","source":"save_predictions(predictions, 'similar4shot_predictions.txt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once again, inspect some of your predictions (from prompting with similar examples) compared to the correct outputs, and describe some common types of errors in your report. Are there any differences compared to the finetuned model, or are the types of errors pretty similar? You can also check the exact match accuracy again.","metadata":{}},{"cell_type":"markdown","source":"Finally, do some open-ended exploration to try to improve your performance on this dataset as much as possible (whether for finetuning or prompting). No hard requirement on how much to improve (or to improve at all), but please discuss the ideas you tried + how effective they were in your report. (Be careful with the GPU memory if you're using Kaggle, though- we're already nearly capping out the GPU memory in a few places with the current settings.)\n\nA non-exhaustive list of possible ideas:\n* Use a different similarity metric for selecting examples in few-shot prompting\n* Use more examples in few-shot prompting\n* Load a different base model than GPT2-Medium, or look into calling the OpenAI API\n* Tune the hyperparameters used for finetuning\n* Try to combine few-shot prompting with finetuning","metadata":{}},{"cell_type":"markdown","source":"Your final submission should include the following files:\n\n* hw4.ipynb (this file; please rename to match)\n* finetuned_predictions.txt\n* similar4shot_predictions.txt\n* report.pdf","metadata":{}}]}