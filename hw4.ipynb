{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 4: Finetuning and Prompting\n\nIn this project, you will first learn how to use Huggingface's Transformers library to load large language models. Next, we will generate text from these models. Finally, we will work with a small text-to-SQL dataset, where the input is a natural language query and the output is a SQL query that can be executed against a database. You will explore (1) finetuning a pretrained language model, and (2) prompting the pretrained language model with examples. \n\nThis project will be more open ended than the previous projects. We expect you to learn how to use the huggingface and torch documentation.","metadata":{}},{"cell_type":"markdown","source":"## Setup\n\nFirst we install and import the required dependencies. These include:\n* `torch` for modeling and training\n* `transformers` for pre-trained models\n* `datasets` from huggingface to load existing datasets.","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install transformers\n!pip install datasets\n!pip install sentence-transformers\n\n# Standard library imports\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:00:34.252552Z","iopub.execute_input":"2025-01-15T08:00:34.252824Z","iopub.status.idle":"2025-01-15T08:01:18.593182Z","shell.execute_reply.started":"2025-01-15T08:00:34.252793Z","shell.execute_reply":"2025-01-15T08:01:18.592200Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU.\nWe'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging.","metadata":{}},{"cell_type":"code","source":"assert torch.cuda.is_available()\ndevice = torch.device(\"cuda\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:02:04.025020Z","iopub.execute_input":"2025-01-15T08:02:04.025396Z","iopub.status.idle":"2025-01-15T08:02:04.107591Z","shell.execute_reply.started":"2025-01-15T08:02:04.025365Z","shell.execute_reply":"2025-01-15T08:02:04.106812Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Loading Model","metadata":{}},{"cell_type":"markdown","source":"We will use GPT-2 Medium for this project. This includes both the GPT-2 tokenizer and the GPT-2 model weights itself. If you want to learn more about this model, you can read the GPT-2 paper https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\n\nLet's first load the tokenizer for the GPT-2 medium model. You can find how to do this by reading the documentation for AutoTokenzier in transformers, and finding the GPT-2 model of ~345 million params in there.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# Your code here\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n\ntokenizer.pad_token = tokenizer.eos_token # convenient for padding later","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:02:11.741466Z","iopub.execute_input":"2025-01-15T08:02:11.741766Z","iopub.status.idle":"2025-01-15T08:02:12.956025Z","shell.execute_reply.started":"2025-01-15T08:02:11.741742Z","shell.execute_reply":"2025-01-15T08:02:12.955357Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7492bea1e9ce41baa5ca97ea8c666476"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cb04e9c29ff4465a14938166cc8cf0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad8a0c5ad764efe872670321bb44185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce885c10d63f4fe387974ed35d752302"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e154dfa2cd144754ab6c2c152e8b0bd4"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Let's tokenize and detokenize some text from this model.","metadata":{}},{"cell_type":"code","source":"print(tokenizer.encode('Hello world'))\nprint(tokenizer.decode(tokenizer.encode('Hello world')))\nprint(tokenizer.encode(\"Hola, cómo estás😍\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:02:19.062782Z","iopub.execute_input":"2025-01-15T08:02:19.063070Z","iopub.status.idle":"2025-01-15T08:02:19.078106Z","shell.execute_reply.started":"2025-01-15T08:02:19.063049Z","shell.execute_reply":"2025-01-15T08:02:19.077315Z"}},"outputs":[{"name":"stdout","text":"[15496, 995]\nHello world\n[39, 5708, 11, 269, 10205, 5908, 1556, 40138, 47249, 235]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Now let's load the GPT-2 Medium model. Make sure you also put the model onto the GPU.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n# Your code here\ngpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:02:23.059712Z","iopub.execute_input":"2025-01-15T08:02:23.060003Z","iopub.status.idle":"2025-01-15T08:02:31.948878Z","shell.execute_reply.started":"2025-01-15T08:02:23.059981Z","shell.execute_reply":"2025-01-15T08:02:31.947859Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f856550c8b834903b630aa6c1461e9e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"077ab9ec358d4d76b52202e3c55c846a"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Generate From the Model","metadata":{}},{"cell_type":"markdown","source":"Now let's generate some text from the model to test its LM capabilities. Let's first generate one output of length 50 tokens using greedy decoding (temperature = 0), which should get us some text with high likelihood under the model. When generating text, you can condition on phrases such as \"The coolest thing in NLP right now is\". Find the relevant function and arguments to use for generating text using the Huggingface documentation.\n\nHint: you may find https://huggingface.co/docs/transformers/main_classes/text_generation to be useful for learning about generating from LMs.","metadata":{}},{"cell_type":"code","source":"inputs = tokenizer(\"The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n# Your code here\n#sample_output = gpt2_model.generate(inputs, do_sample=True, temperature=0.7, top_k=2, max_new_tokens=50)[0]\nattention_mask = (inputs != tokenizer.pad_token_id).float()\nsample_output = gpt2_model.generate(inputs, max_length=50, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, temperature=0, num_return_sequences=1)[0]\nprint(sample_output)\nprint(\"{}\".format(tokenizer.decode(sample_output, skip_special_tokens=True)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:03:35.425637Z","iopub.execute_input":"2025-01-15T08:03:35.425987Z","iopub.status.idle":"2025-01-15T08:03:36.055661Z","shell.execute_reply.started":"2025-01-15T08:03:35.425960Z","shell.execute_reply":"2025-01-15T08:03:36.054952Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"tensor([  464, 38889,  1517,   826,   783,   287,   399, 19930,   318,   262,\n         2694,   284,   779,   262,   976,  1366,   284,  4331,   262,  2003,\n           13,   198,   198,   464,  2003,   318,  1464,  5609,    11,   290,\n          262,  1366,   318,  1464,  5609,    13,   198,   198,   464,  2003,\n          318,  1464,  5609,    11,   290,   262,  1366,   318,  1464,  5609],\n       device='cuda:0')\nThe coolest thing right now in NLP is the ability to use the same data to predict the future.\n\nThe future is always changing, and the data is always changing.\n\nThe future is always changing, and the data is always changing\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Now let's generate 10 pieces of random text of length 50 tokens from the model using random sampling with temperature set to 0.7. This will allow the text to be somewhat higher in diversity (random sampling) while maintaining reasonable quality (temperature < 1). ","metadata":{}},{"cell_type":"code","source":"inputs = tokenizer(\"The coolest thing right now in NLP is\", return_tensors=\"pt\").input_ids.cuda()\n# Your code here\n#sample_outputs = gpt2_model.generate(inputs, num_return_sequences=10, do_sample=True, temperature=0.7, top_k=2, max_new_tokens=50)\nattention_mask = (inputs != tokenizer.pad_token_id).float()\nsample_outputs = gpt2_model.generate(inputs, max_length=50, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, temperature=0.7, num_return_sequences=10, do_sample=True)\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:04:16.554753Z","iopub.execute_input":"2025-01-15T08:04:16.555043Z","iopub.status.idle":"2025-01-15T08:04:17.266668Z","shell.execute_reply.started":"2025-01-15T08:04:16.555022Z","shell.execute_reply":"2025-01-15T08:04:17.265856Z"}},"outputs":[{"name":"stdout","text":"0: The coolest thing right now in NLP is the ability to turn any data set into an NLP model, and to create custom NLP models. I think that's going to be the most important thing to come along in the future.\n\n\n1: The coolest thing right now in NLP is that we have a bunch of awesome techniques that we can use to reduce the amount of time we spend talking to other users about a query.\"\n\nHe continued, \"It's not like we're going\n2: The coolest thing right now in NLP is that you can start seeing the real world in terms of a very simple neural network. You can see things like the visual cortex and different parts of the brain work together, and it's not just the brain\n3: The coolest thing right now in NLP is that it can help you discover the underlying mechanisms of cognitive processes. Now, we can compare these processes and predict how they'll change over time. For example, by understanding how memories are stored, we can\n4: The coolest thing right now in NLP is the use of object recognition for machine learning, which is really exciting. I'm also working on a book about NLP and machine learning called NLP: The Art of Intelligence. But if you're just\n5: The coolest thing right now in NLP is the deep learning data mining that is happening. If you look at what are the top 10 open source frameworks, there are a few that stand out. I think for me it's the deep learning framework,\n6: The coolest thing right now in NLP is the use of data mining. In fact, I believe we should be using them to get a better understanding of our world, and our problems.\n\nThe new wave of NLP software is very helpful\n7: The coolest thing right now in NLP is the idea of being able to build models with your mind, and make them interactive. That's really exciting and I'm really excited about it.\"\n\n\"There are already a lot of really cool projects\n8: The coolest thing right now in NLP is the new algorithm called the Neural Network Optimization Engine. It's a way to model neural networks that are already used in a lot of other problems. But it's different than a traditional one because it's\n9: The coolest thing right now in NLP is the deep learning ecosystem, and the fact that the big three companies are starting to come out with deep learning APIs, I mean I'm a big believer in that part of the ecosystem, but I think we\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Text-to-SQL Task Setup","metadata":{}},{"cell_type":"markdown","source":"First, let's download the data of text-to-SQL pairs and the database against which we'll execute queries to retrieve answers.\n\nThe code below initializes the database and does some initial preprocessing data preprocessing + splitting for you.","metadata":{}},{"cell_type":"code","source":"%%capture\n!wget https://github.com/jkkummerfeld/text2sql-data/raw/master/data/geography.json\n!wget https://github.com/jkkummerfeld/text2sql-data/raw/master/data/geography-db.sql\n\nimport re\nimport sqlite3\nimport json\nfrom copy import deepcopy\n\nDATABASE_NAME = 'geo.db'\nSQL_FILENAME = 'geography-db.sql'\nDATASET_FILENAME = 'geography.json'\n\nwith open(SQL_FILENAME, 'r') as file:\n    sql_script = file.read()\n    sql_script = re.sub(r\"\\s*ENGINE=[^ ]+\",\"\", sql_script)\n    sql_script = re.sub(r\"\\s*DEFAULT CHARSET=[^ ;]+\",\"\", sql_script)\n    sql_script = re.sub(r\"\\s*LOCK TABLES `[^`]+` WRITE;\",\"\", sql_script)  # remove LOCK TABLES\n    sql_script = re.sub(r\"\\s*UNLOCK TABLES;\",\"\", sql_script)  # remove UNLOCK TABLES\n    sql_script = sql_script.replace('`', '')  # remove backticks\n\n# Connect to the SQLite database (this will create the file if it doesn't exist)\nconnection = sqlite3.connect(DATABASE_NAME)\nprint(sql_script)\n\nconnection.executescript(sql_script)\nconnection.commit()\nconnection.close()\n\nconnection = sqlite3.connect(DATABASE_NAME)\ncursor = connection.cursor()\nwith open(DATASET_FILENAME, 'r') as file:\n    dataset = json.load(file)\n\nsplits = {'train': [], 'dev': [], 'test': []}\nfor query_type in dataset:\n    for example in query_type['sentences']:\n        split = example['question-split']\n        example['question'] = example['text']\n        for key, value in example['variables'].items():\n            example['question'] = example['question'].replace(key, value)\n        example['sql'] = deepcopy(query_type['sql'])\n        example['sql'] = example['sql'][0]\n        for key, value in example['variables'].items():\n            example['sql'] = example['sql'].replace(key, value)\n        try:\n            cursor.execute(example['sql'])\n        except:\n            continue\n        example['db_answer'] = cursor.fetchall()\n        del example['text']\n        del example['variables']\n        splits[split].append(example)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:04:30.751983Z","iopub.execute_input":"2025-01-15T08:04:30.752323Z","iopub.status.idle":"2025-01-15T08:04:32.312433Z","shell.execute_reply.started":"2025-01-15T08:04:30.752293Z","shell.execute_reply":"2025-01-15T08:04:32.311565Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"We also provide a function you can use to query the database:","metadata":{}},{"cell_type":"code","source":"def query_db(sql):\n    connection = sqlite3.connect(DATABASE_NAME)\n    cursor = connection.cursor()\n    cursor.execute(sql)\n    result = cursor.fetchall()\n    connection.close()\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:04:46.236439Z","iopub.execute_input":"2025-01-15T08:04:46.236798Z","iopub.status.idle":"2025-01-15T08:04:46.241013Z","shell.execute_reply.started":"2025-01-15T08:04:46.236765Z","shell.execute_reply":"2025-01-15T08:04:46.240339Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"This dataset is pretty small:","metadata":{}},{"cell_type":"code","source":"print('Train set size:', len(splits['train']))\nprint('Dev set size:', len(splits['dev']))\nprint('Test set size:', len(splits['test']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:04:49.473040Z","iopub.execute_input":"2025-01-15T08:04:49.473423Z","iopub.status.idle":"2025-01-15T08:04:49.478540Z","shell.execute_reply.started":"2025-01-15T08:04:49.473390Z","shell.execute_reply":"2025-01-15T08:04:49.477828Z"}},"outputs":[{"name":"stdout","text":"Train set size: 547\nDev set size: 48\nTest set size: 277\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Let's inspect an example from the training dataset:","metadata":{}},{"cell_type":"code","source":"splits['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:04:53.580677Z","iopub.execute_input":"2025-01-15T08:04:53.580971Z","iopub.status.idle":"2025-01-15T08:04:53.586462Z","shell.execute_reply.started":"2025-01-15T08:04:53.580948Z","shell.execute_reply":"2025-01-15T08:04:53.585667Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'question-split': 'train',\n 'question': 'what is the biggest city in nebraska',\n 'sql': 'SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"nebraska\" ) AND CITYalias0.STATE_NAME = \"nebraska\" ;',\n 'db_answer': [('omaha',)]}"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"Note that the `db_answer` is the result of executing the given SQL output against the database:","metadata":{}},{"cell_type":"code","source":"query_db(splits['train'][0]['sql'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:04:56.997568Z","iopub.execute_input":"2025-01-15T08:04:56.997866Z","iopub.status.idle":"2025-01-15T08:04:57.003771Z","shell.execute_reply.started":"2025-01-15T08:04:56.997843Z","shell.execute_reply":"2025-01-15T08:04:57.002924Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[('omaha',)]"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"Let's check how well our language model does on this text-to-SQL task out of the box. You can just use greedy decoding. ","metadata":{}},{"cell_type":"code","source":"prompt = \"Write a SQL query based on the following question.\\n\\nQuestion: {input}\\n\\nSQL:\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:05:30.194731Z","iopub.execute_input":"2025-01-15T08:05:30.195154Z","iopub.status.idle":"2025-01-15T08:05:30.198751Z","shell.execute_reply.started":"2025-01-15T08:05:30.195112Z","shell.execute_reply":"2025-01-15T08:05:30.197956Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Your code here. Generate from the model using greedy decoding with the above prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n#output = gpt2_model.generate(inputs, do_sample=True, temperature=0.7, top_k=2, max_new_tokens=50)[0]\n#output = gpt2_model.generate(inputs, max_length=50, temperature=0, num_return_sequences=1)[0]\nattention_mask = (inputs != tokenizer.pad_token_id).float()\noutput = gpt2_model.generate(inputs, max_length=50, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, temperature=0, num_return_sequences=1)[0]\npredicted_sql = tokenizer.decode(output, skip_special_tokens=True) \nprint(predicted_sql)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:06:03.725884Z","iopub.execute_input":"2025-01-15T08:06:03.726188Z","iopub.status.idle":"2025-01-15T08:06:04.174933Z","shell.execute_reply.started":"2025-01-15T08:06:03.726164Z","shell.execute_reply":"2025-01-15T08:06:04.174244Z"}},"outputs":[{"name":"stdout","text":"Write a SQL query based on the following question.\n\nQuestion: {input}\n\nSQL: SELECT * FROM {input}\n\nOutput:\n\nSELECT * FROM {input}\n\nQuestion: {input}\n\nSQL:\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"You should get something that looks kind of like a SQL query, but it probably won't match the correct output, and in fact it most likely won't even execute without crashing when you try to query the database (you'll see a syntax error below).","metadata":{}},{"cell_type":"code","source":"try:\n    query_db(predicted_sql)\n    print('success!')\nexcept:\n    print('failed to execute!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:06:13.181187Z","iopub.execute_input":"2025-01-15T08:06:13.181563Z","iopub.status.idle":"2025-01-15T08:06:13.186302Z","shell.execute_reply.started":"2025-01-15T08:06:13.181533Z","shell.execute_reply":"2025-01-15T08:06:13.185432Z"}},"outputs":[{"name":"stdout","text":"failed to execute!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Let's confirm quantitatively that the model doesn't work well out-of-the-box by running on the dev dataset (`splits['dev']`).","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef predict_greedy(model, data, max_new_tokens=128):\n    \"\"\"\n    Return the model's greedy text-to-sql predictions on the given data split.\n    The maximum number of new tokens generated (NOT including tokens in the prompt) should be equal to max_new_tokens.\n    For speed, you should batch the generation. The tokenizer can handle multiple inputs simultaneously,\n    but you'll need to tell it to pad using padding=True, and you may also need to set tokenizer.padding_side='left'.\n    Hint: as a postprocessing step after you're done, you may need to cut off the output at the first appearance of '\\n' if the output is continuing past the end of the SQL.\n    \"\"\"\n    questions = [d['question'] for d in data]\n    predicted_sqls = []\n    # Your code here\n    batch_size = 4\n    model = model.to(device)\n    model.eval()\n    tokenizer.padding_side='left'\n    PROMPT = \"Write a SQL query based on the following question.\\n Question: {question}\\n\\nSQL:\"\n    questions=[PROMPT.format(question=i) for i in questions]\n    for i in range(0, len(questions), batch_size):\n        batch_questions = questions[i:i + batch_size]\n        inputs = tokenizer(batch_questions, return_tensors=\"pt\", padding=True).input_ids.cuda()\n        attention_mask = (torch.ones_like(inputs)).float()\n        output = model.generate(inputs, max_length=max_new_tokens,  pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, num_return_sequences=1, temperature=0,do_sample=False)\n        generated_sqls = tokenizer.batch_decode(output, skip_special_tokens=True, max_length=max_new_tokens)\n        \n        for i in range(len(generated_sqls)):\n            if '\\n\\n' in generated_sqls[i]:\n                components = generated_sqls[i].split('\\n\\n')\n                predicted_sqls.append(components[1].replace(\"SQL: \", \"\"))\n    return predicted_sqls # list of strings containing SQL predictions for each question in the data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:07:35.442559Z","iopub.execute_input":"2025-01-15T08:07:35.442975Z","iopub.status.idle":"2025-01-15T08:07:35.449873Z","shell.execute_reply.started":"2025-01-15T08:07:35.442942Z","shell.execute_reply":"2025-01-15T08:07:35.449108Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def check_execution_accuracy(predictions, data):\n    assert len(predictions) == len(data)\n    correct = 0\n    for p, d in zip(predictions, data):\n        try:\n            if query_db(p) == d['db_answer']:\n                correct += 1\n        except: # failed to execute\n            pass\n    return correct / len(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:07:42.684663Z","iopub.execute_input":"2025-01-15T08:07:42.684953Z","iopub.status.idle":"2025-01-15T08:07:42.689588Z","shell.execute_reply.started":"2025-01-15T08:07:42.684930Z","shell.execute_reply":"2025-01-15T08:07:42.688673Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"predictions = predict_greedy(gpt2_model, splits['dev'])\nprint('example prediction:', predictions[0])\nprint('initial execution acc', check_execution_accuracy(predictions, splits['dev']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:07:49.783585Z","iopub.execute_input":"2025-01-15T08:07:49.783867Z","iopub.status.idle":"2025-01-15T08:08:08.210380Z","shell.execute_reply.started":"2025-01-15T08:07:49.783844Z","shell.execute_reply":"2025-01-15T08:08:08.209507Z"}},"outputs":[{"name":"stdout","text":"example prediction: SELECT city FROM city WHERE city.name = 'Arizona' AND city.zipcode = 'AZ'\ninitial execution acc 0.0\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"You will probably observe an accuracy around 0-2%. (It may be hard to verify if your `predict_greedy` function is correct at this stage, because the expected accuracy is so low, but you will reuse it later with an improved model, at which point it will be more obvious if your implementation is correct.)","metadata":{}},{"cell_type":"markdown","source":"## Model Finetuning","metadata":{}},{"cell_type":"markdown","source":"Now let's prepare our dataset for finetuning (i.e., training our pretrained language model on this text-to-SQL training set). For each element in the dataset, it should have a text prompt and then the SQL output, similar to above. Your job is to fill in the labels field below. This field sets the labels to use for training during the language modeling task.\n\nFor the labels, we only want to train the model to output the text after the word \"SQL:\". This is because in the prompt, everything before the word \"SQL:\" will also be provided to the model as input. Hint: use -100 as the label for tokens you do not want to train on. Hint 2: When doing LM training, the labels are the same as the input tokens, except shifted to the left by one. You should check whether Huggingface is already doing the shifting, or whether you need to do the shifting yourself.\n\nOne thing to be careful of with all LMs is to make sure there are not extra spaces. So, the text should be formatted as like \"SQL: {sql output}\" not \"SQL: {sql output} \". ","metadata":{}},{"cell_type":"code","source":"class Text2SQLDataset(Dataset):\n    PROMPT = \"Write a SQL query based on the following question.\\n\\nQuestion: {question}\\n\\nSQL: {sql}\"\n\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n        tokenizer.padding_side = 'right'\n\n        self.input_ids = []\n        self.attn_masks = []\n        self.labels = []\n\n        training_texts = []\n        for example in self.data:\n            training_text = Text2SQLDataset.PROMPT.format(question=example['question'], sql=example['sql']) + \"<|endoftext|>\" # include the end token so model knows when to stop!\n            training_texts.append(training_text)\n        encodings_dict = self.tokenizer(training_texts, padding=True, truncation=True)\n        for i, (example, training_text) in enumerate(zip(data, training_texts)):\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids'][i]))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask'][i]))\n            # Your code here\n            input_tokens = self.tokenizer.tokenize(training_text)\n            sql_start = input_tokens.index('SQL') \n            label = encodings_dict['input_ids'][i]\n            label[:sql_start]=[-100]*sql_start\n            self.labels.append(torch.tensor(label))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {'input_ids':(self.input_ids[idx]),\n                'attn_masks': (self.attn_masks[idx]),\n                'labels': (self.labels[idx])}\n        #return self.input_ids[idx], self.attn_masks[idx], self.labels[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:09:01.979880Z","iopub.execute_input":"2025-01-15T08:09:01.980195Z","iopub.status.idle":"2025-01-15T08:09:01.986912Z","shell.execute_reply.started":"2025-01-15T08:09:01.980172Z","shell.execute_reply":"2025-01-15T08:09:01.986151Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train_dataset = Text2SQLDataset(splits['train'], tokenizer)\ndev_dataset = Text2SQLDataset(splits['dev'], tokenizer)\ntest_dataset = Text2SQLDataset(splits['test'], tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:09:07.797648Z","iopub.execute_input":"2025-01-15T08:09:07.797994Z","iopub.status.idle":"2025-01-15T08:09:08.243919Z","shell.execute_reply.started":"2025-01-15T08:09:07.797961Z","shell.execute_reply":"2025-01-15T08:09:08.243235Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"Now we can use the Huggingface Trainer to finetune GPT-2 Medium on this dataset. This abstracts away all of the details of training. Setup the training arguments to perform 3 epochs of training on this dataset, use a per-device batch size of 2 with gradient accumulation set to 8, use 30 warmup steps, a weight decay of 0.05. Set the eval batch size to be 8. Save a checkpoint after 100 steps. Set fp16 to True. Save the checkpoint in a specific output_dir so you can load it later. Hint: if it tries to launch Wandb, you may add the argument report_to=\"none\".","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, DataCollatorWithPadding\n\n# Define the output directory to save checkpoints\noutput_dir = \"/kaggle/working/checkpoint\"\n\ntraining_args = TrainingArguments(\n    output_dir=\"./output\",\n    overwrite_output_dir=True,  \n    num_train_epochs=3,  \n    per_device_train_batch_size=2,  \n    gradient_accumulation_steps=8,\n    warmup_steps=30, \n    weight_decay=0.05, \n    per_device_eval_batch_size=8,\n    #save_steps=100, \n    evaluation_strategy=\"steps\",  \n    eval_steps=10,  \n    save_total_limit=2,  \n    report_to=\"none\", \n    fp16=True,   \n    load_best_model_at_end=True,  \n    logging_steps=100,\n    save_safetensors=False\n)\n\ndata_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer=Trainer(\n    model=gpt2_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    eval_dataset=dev_dataset\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:16:05.851634Z","iopub.execute_input":"2025-01-15T08:16:05.851937Z","iopub.status.idle":"2025-01-15T08:21:17.133183Z","shell.execute_reply.started":"2025-01-15T08:16:05.851914Z","shell.execute_reply":"2025-01-15T08:21:17.132302Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-24-158a32dd7376>:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer=Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [51/51 05:04, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>0.031616</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>No log</td>\n      <td>0.028785</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>No log</td>\n      <td>0.026199</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>No log</td>\n      <td>0.021237</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>No log</td>\n      <td>0.020057</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=51, training_loss=0.020360575002782485, metrics={'train_runtime': 310.5934, 'train_samples_per_second': 5.283, 'train_steps_per_second': 0.164, 'total_flos': 862218784653312.0, 'train_loss': 0.020360575002782485, 'epoch': 2.875912408759124})"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"Reload the final saved version of the model below. You may need to delete the previously loaded model if you run out of GPU memory.","metadata":{}},{"cell_type":"code","source":"#del gpt2_model\n# Your code here\nfinetuned_model = AutoModelForCausalLM.from_pretrained('/kaggle/working/output/checkpoint-51')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:22:13.458694Z","iopub.execute_input":"2025-01-15T08:22:13.458998Z","iopub.status.idle":"2025-01-15T08:22:13.614927Z","shell.execute_reply.started":"2025-01-15T08:22:13.458973Z","shell.execute_reply":"2025-01-15T08:22:13.614088Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"Let's check our finetuned model's performance.","metadata":{}},{"cell_type":"code","source":"finetuned_predictions = predict_greedy(finetuned_model, splits['test'])\nprint('finetuned execution acc:', check_execution_accuracy(finetuned_predictions, splits['test']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:22:24.064175Z","iopub.execute_input":"2025-01-15T08:22:24.064525Z","iopub.status.idle":"2025-01-15T08:23:27.645228Z","shell.execute_reply.started":"2025-01-15T08:22:24.064500Z","shell.execute_reply":"2025-01-15T08:23:27.644311Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"finetuned execution acc: 0.5054151624548736\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"You should achieve an accuracy of roughly 50% using the suggested training hyperparameters; we will check >40% in the autograder.","metadata":{}},{"cell_type":"markdown","source":"Save your predictions.","metadata":{}},{"cell_type":"code","source":"def save_predictions(predictions, filename):\n    with open(filename, 'w') as f:\n        f.write('\\n'.join(predictions))\n\nsave_predictions(finetuned_predictions, 'finetuned_predictions.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:23:40.874019Z","iopub.execute_input":"2025-01-15T08:23:40.874382Z","iopub.status.idle":"2025-01-15T08:23:40.879045Z","shell.execute_reply.started":"2025-01-15T08:23:40.874350Z","shell.execute_reply":"2025-01-15T08:23:40.878313Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"Inspect some of your predictions compared to the correct outputs, and describe some common types of errors in your report. What fraction of errors are due to failing to execute (e.g., syntax error), and what fraction are due to executing but getting the wrong answer?","metadata":{}},{"cell_type":"code","source":"# Your code here\nlen(finetuned_predictions),len(splits['test'])\ncnt=0\nfor i in range(len(finetuned_predictions)):\n    if finetuned_predictions[i]!=splits['test'][i]['sql']:\n        cnt+=1\n        print(i,finetuned_predictions[i],\"\\n\",splits['test'][i]['sql'])\nprint(cnt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:24:13.304872Z","iopub.execute_input":"2025-01-15T08:24:13.305163Z","iopub.status.idle":"2025-01-15T08:24:13.378150Z","shell.execute_reply.started":"2025-01-15T08:24:13.305141Z","shell.execute_reply":"2025-01-15T08:24:13.377219Z"}},"outputs":[{"name":"stdout","text":"4 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"new mexico\" ) AND CITYalias0.STATE_NAME = \"new mexico\" ;\n23 SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"washington\" AND CITYalias0.STATE_NAME = \"washington\" ; \n SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"washington\" ;\n24 SELECT STATEalias0.LOWEST_POINT FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"maine\" ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MIN( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ;\n25 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME = \"texas\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.STATE_NAME = \"texas\" ;\n26 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"missouri\" ; \n SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"missouri\" ;\n31 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"chattahoochee\" ) AND RIVERalias0.TRAVERSE = \"chattahoochee\" ; \n SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"chattahoochee\" ;\n34 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ;\n35 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ;\n39 SELECT DISTINCT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"longest\" ; \n SELECT DISTINCT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 ) ;\n42 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"california\" ) AND RIVERalias0.TRAVERSE = \"california\" ; \n SELECT COUNT( RIVERalias0.RIVER_NAME ) FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"california\" ;\n53 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"kentucky\" ;\n63 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"san antonio\" ;\n70 SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"salt lake\" ; \n SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"salt lake city\" ;\n83 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"longest\" ) AND RIVERalias0.RIVER_NAME = \"longest\" ; \n SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 ) ;\n85 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"montana\" ; \n SELECT HIGHLOWalias0.HIGHEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"montana\" ;\n87 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"nevada\" ; \n SELECT HIGHLOWalias0.HIGHEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"nevada\" ;\n88 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"usa\" ) AND RIVERalias0.TRAVERSE = \"usa\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 ) ;\n91 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"georgia\" ; \n SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"georgia\" ) ORDER BY HIGHLOWalias0.HIGHEST_ELEVATION DESC LIMIT 1 ;\n92 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"colorado\" ; \n SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"colorado\" ) ORDER BY HIGHLOWalias0.HIGHEST_ELEVATION DESC LIMIT 1 ;\n103 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT COUNT( HIGHLOWalias0.STATE_NAME ) FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.LOWEST_ELEVATION < ( SELECT HIGHLOWalias1.LOWEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias1 WHERE HIGHLOWalias1.STATE_NAME = \"alabama\" ) ;\n104 SELECT HIGHLOWalias0.HIGHEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"mckinley\" ; \n SELECT HIGHLOWalias0.HIGHEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_POINT = \"mount mckinley\" ;\n105 SELECT MAX( MOUNTAINalias0.MOUNTAIN_ALTITUDE ) FROM MOUNTAIN AS MOUNTAINalias0 WHERE MOUNTAINalias0.STATE_NAME = \"mckinley\" ; \n SELECT HIGHLOWalias0.HIGHEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_POINT = \"mount mckinley\" ;\n106 SELECT MAX( CITYalias0.LOWEST_ELEVATION ) FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"san francisco\" ; \n SELECT HIGHLOWalias0.HIGHEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_POINT = \"san francisco\" ;\n107 SELECT HIGHLOWalias0.HIGHEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"america\" ; \n SELECT MAX( HIGHLOWalias0.HIGHEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias0 ;\n108 SELECT HIGHLOWalias0.HIGHEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"united states\" ; \n SELECT MAX( HIGHLOWalias0.HIGHEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias0 ;\n110 SELECT DERIVED_TABLEalias0.LENGTH FROM DERIVED_TABLE AS DERIVED_TABLEalias0 WHERE DERIVED_TABLEalias0.STATE_NAME = \"delaware\" ; \n SELECT DISTINCT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"delaware\" ;\n113 SELECT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"colorado\" ; \n SELECT DISTINCT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"colorado\" ;\n114 SELECT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"mississippi\" ; \n SELECT DISTINCT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"mississippi\" ;\n115 SELECT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"mississippi\" ; \n SELECT DISTINCT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"mississippi\" ;\n116 SELECT DISTINCT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"longest\" ; \n SELECT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"california\" ) AND RIVERalias0.TRAVERSE = \"california\" ;\n117 SELECT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"texas\" ; \n SELECT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"texas\" ) AND RIVERalias0.TRAVERSE = \"texas\" ;\n118 SELECT COUNT( CITYalias0.CAPITAL ) FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"rhode island\" ; \n SELECT COUNT( STATEalias0.CAPITAL ) FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"rhode island\" ;\n119 SELECT COUNT( CITYalias0.CITY_NAME ) FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME = \"united states\" ; \n SELECT COUNT( CITYalias0.CITY_NAME ) FROM CITY AS CITYalias0 ;\n120 SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"big city\" ; \n SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 ) ;\n121 SELECT COUNT( RIVERalias0.RIVER_NAME ) FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"colorado\" ; \n SELECT COUNT( RIVERalias0.RIVER_NAME ) FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"colorado\" ;\n123 SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"erie\" ; \n SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"erie\" AND CITYalias0.STATE_NAME = \"pennsylvania\" ;\n125 SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"new york\" ; \n SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"new york\" ) AND CITYalias0.STATE_NAME = \"new york\" ;\n126 SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"texas\" ; \n SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = ( SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"texas\" ) ;\n127 SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"united states\" ; \n SELECT SUM( STATEalias0.POPULATION ) FROM STATE AS STATEalias0 ;\n128 SELECT COUNT( RIVERalias0.RIVER_NAME ) FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"state\" ; \n SELECT COUNT( RIVERalias0.RIVER_NAME ) FROM RIVER AS RIVERalias0 GROUP BY RIVERalias0.TRAVERSE ORDER BY COUNT( RIVERalias0.RIVER_NAME ) DESC LIMIT 1 ;\n129 SELECT COUNT( BORDER_INFOalias0.BORDER ) FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"usa\" ; \n SELECT COUNT( STATEalias0.STATE_NAME ) FROM STATE AS STATEalias0 ;\n130 SELECT COUNT( BORDER_INFOalias0.BORDER ) FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"maine\" ; \n SELECT COUNT( STATEalias0.STATE_NAME ) FROM STATE AS STATEalias0 ;\n131 SELECT COUNT( STATEalias0.STATE_NAME ) FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT COUNT( STATEalias0.STATE_NAME ) FROM STATE AS STATEalias0 ;\n132 SELECT COUNT( STATEalias0.STATE_NAME ) FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT COUNT( STATEalias0.STATE_NAME ) FROM STATE AS STATEalias0 ;\n135 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"iowa\" ; \n SELECT COUNT( BORDER_INFOalias0.BORDER ) FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"iowa\" ;\n137 SELECT COUNT( BORDER_INFOalias0.BORDER ) FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ) \n SELECT COUNT( BORDER_INFOalias0.BORDER ) FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ) ;\n138 SELECT COUNT( DISTINCT RIVERalias0.RIVER_NAME ) FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"not-a-rivers\" ; \n SELECT COUNT( DISTINCT STATEalias0.STATE_NAME ) FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME NOT IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 ) ;\n139 SELECT COUNT( HIGHLOWalias0.STATE_NAME ) FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_ELEVATION = ( SELECT MAX( HIGHLOWalias1.HIGHEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 ) ; \n SELECT COUNT( HIGHLOWalias0.STATE_NAME ) FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_ELEVATION > ( SELECT HIGHLOWalias1.HIGHEST_ELEVATION FROM HIGHLOW AS HIGHLOWalias1 WHERE HIGHLOWalias1.STATE_NAME = ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.CAPITAL = ( SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 ) ) ) ) ;\n140 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"florida\" ) AND RIVERalias0.TRAVERSE = \"florida\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH > 750 AND RIVERalias0.TRAVERSE = \"florida\" ;\n141 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"ohio\" ) AND RIVERalias0.TRAVERSE = \"ohio\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH > 750 AND RIVERalias0.TRAVERSE = \"ohio\" ;\n142 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"texas\" ) AND RIVERalias0.RIVER_NAME = \"texas\" ; \n SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"texas\" ) ;\n143 SELECT CITYalias0.CAPITAL FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"texas\" ; \n SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"texas\" ;\n154 SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"missouri\" ) ; \n SELECT STATEalias0.CAPITAL FROM BORDER_INFO AS BORDER_INFOalias0 , STATE AS STATEalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"missouri\" AND STATEalias0.STATE_NAME = BORDER_INFOalias0.BORDER ;\n155 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME = \"mississippi\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"mississippi\" ) ;\n156 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ) ) AND CITYalias0 \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.STATE_NAME IN ( SELECT HIGHLOWalias0.STATE_NAME FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_ELEVATION = ( SELECT MAX( HIGHLOWalias1.HIGHEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 ) ) ;\n157 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"maine\" ; \n SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 ;\n164 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME = \"mississippi\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH > 750 AND RIVERalias0.RIVER_NAME = \"mississippi\" ) ;\n165 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME = \"usa\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 ;\n166 SELECT STATEalias0.DENSITY FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.DENSITY FROM STATE AS STATEalias0 ;\n169 SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"texas\" ) ; \n SELECT STATEalias0.POPULATION FROM BORDER_INFO AS BORDER_INFOalias0 , STATE AS STATEalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"texas\" AND STATEalias0.STATE_NAME = BORDER_INFOalias0.BORDER ;\n170 SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"texas\" AND CITYalias0.STATE_NAME = \"texas\" ; \n SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME = \"texas\" ;\n171 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"new york\" ) AND CITYalias0.STATE_NAME = \"new york\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 ) ;\n172 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"united states\" ) AND CITYalias0.STATE_NAME = \"united states\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 ) ;\n173 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"newcastle\" ) AND CITYalias0.STATE_NAME = \"newcastle\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 ) ;\n174 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"us\" ) AND CITYalias0.STATE_NAME = \"us\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 ) ;\n175 SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.LOWEST_ELEVATION = ( SELECT MIN( STATEalias1.LOWEST_ELEVATION ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.CAPITAL FROM HIGHLOW AS HIGHLOWalias0 , STATE AS STATEalias0 WHERE HIGHLOWalias0.LOWEST_ELEVATION = ( SELECT MIN( HIGHLOWalias1.LOWEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 ) AND STATEalias0.STATE_NAME = HIGHLOWalias0.STATE_NAME ;\n176 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"america\" ) AND CITYalias0.STATE_NAME = \"maine\" ; \n SELECT DISTINCT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 , RIVER AS RIVERalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 , RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = CITYalias1.STATE_NAME ) AND RIVERalias0.TRAVERSE = CITYalias0.STATE_NAME ;\n177 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"us\" ) AND CITYalias0.STATE_NAME = \"us\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 , STATE AS STATEalias0 WHERE STATEalias0.CAPITAL = CITYalias1.CITY_NAME ) ;\n178 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"usa\" ) AND CITYalias0.STATE_NAME = \"usa\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 , STATE AS STATEalias0 WHERE STATEalias0.CAPITAL = CITYalias1.CITY_NAME ) ;\n179 SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.CITY_NAME = \"durham\" ; \n SELECT STATEalias0.CAPITAL FROM CITY AS CITYalias0 , STATE AS STATEalias0 WHERE CITYalias0.CITY_NAME = \"durham\" AND STATEalias0.STATE_NAME = CITYalias0.STATE_NAME ;\n181 SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.DENSITY = ( SELECT MAX( STATEalias1.DENSITY ) FROM STATE AS STATEalias1 ) ; \n SELECT DISTINCT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.DENSITY = ( SELECT MAX( STATEalias1.DENSITY ) FROM STATE AS STATEalias1 ) ;\n183 SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ;\n184 SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.LENGTH = ( SELECT MAX( STATEalias1.LENGTH ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 ) ) ;\n186 SELECT CITYalias0.DENSITY FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"new york\" ; \n SELECT STATEalias0.DENSITY FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = \"new york\" ;\n188 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"mountain\" ; \n SELECT MOUNTAINalias0.MOUNTAIN_NAME FROM MOUNTAIN AS MOUNTAINalias0 WHERE MOUNTAINalias0.MOUNTAIN_ALTITUDE = ( SELECT MAX( MOUNTAINalias1.MOUNTAIN_ALTITUDE ) FROM MOUNTAIN AS MOUNTAINalias1 ) ;\n189 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"us\" ; \n SELECT MOUNTAINalias0.MOUNTAIN_NAME FROM MOUNTAIN AS MOUNTAINalias0 WHERE MOUNTAINalias0.MOUNTAIN_ALTITUDE = ( SELECT MAX( MOUNTAINalias1.MOUNTAIN_ALTITUDE ) FROM MOUNTAIN AS MOUNTAINalias1 ) ;\n190 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"austin\" ; \n SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.CAPITAL = \"austin\" ) ;\n191 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"usa\" ; \n SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_ELEVATION = ( SELECT MAX( HIGHLOWalias1.HIGHEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 ) ;\n192 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"the usa\" ; \n SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_ELEVATION = ( SELECT MAX( HIGHLOWalias1.HIGHEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 ) ;\n193 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"maine\" ; \n SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.DENSITY = ( SELECT MIN( STATEalias1.DENSITY ) FROM STATE AS STATEalias1 ) ) ;\n194 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"mississippi\" ) AND CITYalias0.STATE_NAME = \"mississippi\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME IN ( SELECT RIVERalias1.TRAVERSE FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"mississippi\" ) AND STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 WHERE STATEalias1.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"mississippi\" ) ) ) ORDER BY CITYalias0.POPULATION DESC LIMIT 1 ;\n195 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"usa\" ) AND CITYalias0.STATE_NAME = \"texas\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ) ) AND CITYalias0.STATE_NAME IN ( SELECT STATEalias2.STATE_NAME FROM STATE AS STATEalias2 WHERE STATEalias2.AREA = ( SELECT MIN( STATEalias3.AREA ) FROM STATE AS STATEalias3 ) ) ;\n196 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 WHERE STATEalias1.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"arkansas\" ) ) AND STATEalias0.STATE_NAME IN ( SELECT BORDER_INFOalias1.BORDER FROM BORDER_INFO AS BORDER_INFOalias1 WHERE BORDER_INFOalias1.STATE_NAME = \"arkansas\" ) ;\n197 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 WHERE STATEalias1.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"texas\" ) ) AND STATEalias0.STATE_NAME IN ( SELECT BORDER_INFOalias1.BORDER FROM BORDER_INFO AS BORDER_INFOalias1 WHERE BORDER_INFOalias1.STATE_NAME = \"texas\" ) ;\n198 SELECT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"river\" ; \n SELECT DISTINCT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = ( SELECT RIVER_NAME FROM ( SELECT COUNT( 1 ) AS DERIVED_FIELDalias0 , RIVERalias1.RIVER_NAME FROM RIVER AS RIVERalias1 GROUP BY RIVERalias1.RIVER_NAME ) AS DERIVED_TABLEalias0 WHERE DERIVED_TABLEalias0.DERIVED_FIELDalias0 = ( SELECT MAX( DERIVED_TABLEalias1.DERIVED_FIELDalias1 ) FROM ( SELECT COUNT( 1 ) AS DERIVED_FIELDalias1 , RIVERalias2.RIVER_NAME FROM RIVER AS RIVERalias2 GROUP BY RIVERalias2.RIVER_NAME ) AS DERIVED_TABLEalias1 ) ) ;\n199 SELECT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"river\" ; \n SELECT DISTINCT RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = ( SELECT RIVER_NAME FROM ( SELECT COUNT( 1 ) AS DERIVED_FIELDalias0 , RIVERalias1.RIVER_NAME FROM RIVER AS RIVERalias1 GROUP BY RIVERalias1.RIVER_NAME ) AS DERIVED_TABLEalias0 WHERE DERIVED_TABLEalias0.DERIVED_FIELDalias0 = ( SELECT MAX( DERIVED_TABLEalias1.DERIVED_FIELDalias1 ) FROM ( SELECT COUNT( 1 ) AS DERIVED_FIELDalias1 , RIVERalias2.RIVER_NAME FROM RIVER AS RIVERalias2 GROUP BY RIVERalias2.RIVER_NAME ) AS DERIVED_TABLEalias1 ) ) ;\n200 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"maine\" ) AND RIVERalias0.TRAVERSE = \"maine\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ) ) AND RIVERalias0.TRAVERSE IN ( SELECT STATEalias2.STATE_NAME FROM STATE AS STATEalias2 WHERE STATEalias2.AREA = ( SELECT MAX( STATEalias3.AREA ) FROM STATE AS STATEalias3 ) ) ;\n201 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"nebraska\" ) AND RIVERalias0.TRAVERSE = \"nebraska\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"nebraska\" ) ) AND RIVERalias0.TRAVERSE IN ( SELECT BORDER_INFOalias1.BORDER FROM BORDER_INFO AS BORDER_INFOalias1 WHERE BORDER_INFOalias1.STATE_NAME = \"nebraska\" ) ;\n202 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"indiana\" ) AND RIVERalias0.TRAVERSE = \"indiana\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"indiana\" ) ) AND RIVERalias0.TRAVERSE IN ( SELECT BORDER_INFOalias1.BORDER FROM BORDER_INFO AS BORDER_INFOalias1 WHERE BORDER_INFOalias1.STATE_NAME = \"indiana\" ) ;\n203 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"rIVER\" ) AND RIVERalias0.TRAVERSE = \"city\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = ( SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 GROUP BY CITYalias0.STATE_NAME ORDER BY COUNT( CITYalias0.CITY_NAME ) DESC LIMIT 1 ) ORDER BY RIVERalias0.LENGTH DESC LIMIT 1 ;\n210 SELECT HIGHLOWalias0.LOWEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"usa\" ; \n SELECT HIGHLOWalias0.LOWEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.LOWEST_ELEVATION = ( SELECT MIN( HIGHLOWalias1.LOWEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 ) ;\n211 SELECT HIGHLOWalias0.LOWEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"colorado\" ; \n SELECT HIGHLOWalias0.LOWEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"colorado\" ) ORDER BY HIGHLOWalias0.LOWEST_ELEVATION LIMIT 1 ;\n214 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 WHERE STATEalias1.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"mississippi\" ) ) AND STATEalias0.STATE_NAME IN ( SELECT RIVERalias1.TRAVERSE FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"mississippi\" ) ;\n216 SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"maine\" AND CITYalias0.STATE_NAME = \"maine\" ; \n SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ) ) AND CITYalias0.STATE_NAME IN ( SELECT STATEalias2.STATE_NAME FROM STATE AS STATEalias2 WHERE STATEalias2.AREA = ( SELECT MAX( STATEalias3.AREA ) FROM STATE AS STATEalias3 ) ) ;\n218 SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"maine\" ) ; \n SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 GROUP BY BORDER_INFOalias0.BORDER HAVING COUNT( 1 ) = ( SELECT MAX( DERIVED_TABLEalias0.DERIVED_FIELDalias0 ) FROM ( SELECT BORDER_INFOalias1.BORDER , COUNT( 1 ) AS DERIVED_FIELDalias0 FROM BORDER_INFO AS BORDER_INFOalias1 GROUP BY BORDER_INFOalias1.BORDER ) AS DERIVED_TABLEalias0 ) ) ;\n220 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"usa\" ) AND CITYalias0.STATE_NAME = \"usa\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 ) ;\n221 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"texas\" ) AND CITYalias0.STATE_NAME = \"texas\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 ) ;\n222 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 WHERE STATEalias1.STATE_NAME IN ( SELECT DERIVED_TABLEalias0.STATE_NAME FROM ( SELECT BORDER_INFOalias0.STATE_NAME , COUNT( DISTINCT BORDER_INFOalias0.BORDER ) AS DERIVED_FIELDalias0 FROM BORDER_INFO AS BORDER_INFOalias0 GROUP BY BORDER_INFOalias0.STATE_NAME ) AS DERIVED_TABLEalias0 WHERE DERIVED_TABLEalias0.DERIVED_FIELDalias0 = ( SELECT MAX( DERIVED_TABLEalias1.DERIVED_FIELDalias1 ) FROM ( SELECT BORDER_INFOalias1.STATE_NAME , COUNT( DISTINCT BORDER_INFOalias1.BORDER ) AS DERIVED_FIELDalias1 FROM BORDER_INFO AS BORDER_INFOalias1 GROUP BY BORDER_INFOalias1.STATE_NAME ) AS DERIVED_TABLEalias1 ) ) ) ;\n223 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 WHERE STATEalias1.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"wyoming\" ) ) AND STATEalias0.STATE_NAME IN ( SELECT BORDER_INFOalias1.BORDER FROM BORDER_INFO AS BORDER_INFOalias1 WHERE BORDER_INFOalias1.STATE_NAME = \"wyoming\" ) ;\n224 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MIN( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"mississippi\" ) AND RIVERalias0.RIVER_NAME = \"mississippi\" ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 WHERE STATEalias1.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"mississippi\" ) ) AND STATEalias0.STATE_NAME IN ( SELECT RIVERalias1.TRAVERSE FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"mississippi\" ) ;\n228 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"usa\" ) AND RIVERalias0.TRAVERSE = \"usa\" ; \n SELECT SUM( DERIVED_TABLEalias0.LENGTH ) FROM ( SELECT DISTINCT RIVERalias0.RIVER_NAME , RIVERalias0.LENGTH FROM RIVER AS RIVERalias0 ) AS DERIVED_TABLEalias0 ;\n229 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"rIVER_name\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 GROUP BY ( RIVERalias0.RIVER_NAME ) ORDER BY COUNT( DISTINCT RIVERalias0.TRAVERSE ) DESC LIMIT 1 ;\n230 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"ranger\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 GROUP BY ( RIVERalias0.RIVER_NAME ) ORDER BY COUNT( DISTINCT RIVERalias0.TRAVERSE ) DESC LIMIT 1 ;\n231 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"rutgers\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 GROUP BY ( RIVERalias0.RIVER_NAME ) ORDER BY COUNT( DISTINCT RIVERalias0.TRAVERSE ) DESC LIMIT 1 ;\n232 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"rIVER1\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 GROUP BY ( RIVERalias0.RIVER_NAME ) ORDER BY COUNT( DISTINCT RIVERalias0.TRAVERSE ) DESC LIMIT 1 ;\n233 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"rIVER_name\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE IN ( SELECT DERIVED_TABLEalias0.STATE_NAME FROM ( SELECT CITYalias0.STATE_NAME , COUNT( 1 ) AS DERIVED_FIELDalias0 FROM CITY AS CITYalias0 GROUP BY CITYalias0.STATE_NAME ) AS DERIVED_TABLEalias0 WHERE DERIVED_TABLEalias0.DERIVED_FIELDalias0 = ( SELECT MAX( DERIVED_TABLEalias1.DERIVED_FIELDalias1 ) FROM ( SELECT COUNT( 1 ) AS DERIVED_FIELDalias1 FROM CITY AS CITYalias1 GROUP BY CITYalias1.STATE_NAME ) AS DERIVED_TABLEalias1 ) ) ;\n234 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"texas\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"texas\" ) ;\n235 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"ranches\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 GROUP BY BORDER_INFOalias0.BORDER HAVING COUNT( 1 ) = ( SELECT MAX( DERIVED_TABLEalias0.DERIVED_FIELDalias0 ) FROM ( SELECT BORDER_INFOalias1.BORDER , COUNT( 1 ) AS DERIVED_FIELDalias0 FROM BORDER_INFO AS BORDER_INFOalias1 GROUP BY BORDER_INFOalias1.BORDER ) AS DERIVED_TABLEalias0 ) ) ;\n236 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"washington\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 GROUP BY BORDER_INFOalias0.BORDER HAVING COUNT( 1 ) = ( SELECT MAX( DERIVED_TABLEalias0.DERIVED_FIELDalias0 ) FROM ( SELECT BORDER_INFOalias1.BORDER , COUNT( 1 ) AS DERIVED_FIELDalias0 FROM BORDER_INFO AS BORDER_INFOalias1 GROUP BY BORDER_INFOalias1.BORDER ) AS DERIVED_TABLEalias0 ) ) ;\n237 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"nevada\" ) ORDER BY STATEalias0.POPULATION DESC LIMIT 1 ;\n238 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"pennsylvania\" ) ORDER BY STATEalias0.POPULATION DESC LIMIT 1 ;\n239 SELECT HIGHLOWalias0.STATE_NAME FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_ELEVATION = ( SELECT MAX( HIGHLOWalias1.HIGHEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 ) ; \n SELECT HIGHLOWalias0.STATE_NAME FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.HIGHEST_ELEVATION = ( SELECT MAX( HIGHLOWalias1.HIGHEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 WHERE HIGHLOWalias1.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"colorado\" ) ) ;\n240 SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = ( SELECT MAX( CITYalias1.CITY_NAME ) FROM CITY AS CITYalias1 WHERE CITYalias1.POPULATION = ( SELECT MAX( CITYalias2.POPULATION ) FROM CITY AS CITYalias2 WHERE CITYalias2.STATE_NAME = CITYalias1.STATE_NAME ) AND CITYalias0.STATE_NAME = CITYalias2. \n SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 , STATE AS STATEalias0 WHERE STATEalias0.CAPITAL = CITYalias1.CITY_NAME ) ;\n241 SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CAPITAL = \"capital\" ; \n SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 , STATE AS STATEalias0 WHERE STATEalias0.CAPITAL = CITYalias1.CITY_NAME ) ;\n242 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"longest\" ) AND RIVERalias0.RIVER_NAME = \"longest\" ; \n SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( DISTINCT RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 ) ;\n243 SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"new york\" ; \n SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 GROUP BY CITYalias0.STATE_NAME ORDER BY COUNT( 1 ) DESC LIMIT 1 ;\n244 SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = \"california\" ; \n SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 GROUP BY CITYalias0.STATE_NAME ORDER BY COUNT( 1 ) DESC LIMIT 1 ;\n245 SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.CITY_NAME = \"newcastle\" ) AND CITYalias0.STATE_NAME = \"newcastle\" ; \n SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 GROUP BY CITYalias0.STATE_NAME ORDER BY SUM( CITYalias0.POPULATION ) LIMIT 1 ;\n246 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"mississippi\" ; \n SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME IN ( SELECT BORDER_INFOalias1.BORDER FROM BORDER_INFO AS BORDER_INFOalias1 WHERE BORDER_INFOalias1.STATE_NAME = \"mississippi\" ) ;\n247 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"ohio\" ; \n SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"ohio\" ) ;\n248 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"texas\" ; \n SELECT BORDER_INFOalias0.STATE_NAME FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.BORDER = \"texas\" AND BORDER_INFOalias0.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH > 750 ) ;\n249 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"maine\" ; \n SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = ( SELECT MAX( STATEalias1.POPULATION ) FROM STATE AS STATEalias1 ) ) ;\n250 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"maine\" ; \n SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME IN ( SELECT BORDER_INFOalias1.BORDER FROM BORDER_INFO AS BORDER_INFOalias1 GROUP BY BORDER_INFOalias1.BORDER HAVING COUNT( 1 ) = ( SELECT MAX( DERIVED_TABLEalias0.DERIVED_FIELDalias0 ) FROM ( SELECT BORDER_INFOalias2.BORDER , COUNT( 1 ) AS DERIVED_FIELDalias0 FROM BORDER_INFO AS BORDER_INFOalias2 GROUP BY BORDER_INFOalias2.BORDER ) AS DERIVED_TABLEalias0 ) ) ;\n251 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"maine\" ; \n SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME IN ( SELECT DERIVED_TABLEalias0.STATE_NAME FROM ( SELECT CITYalias0.STATE_NAME , COUNT( 1 ) AS DERIVED_FIELDalias0 FROM CITY AS CITYalias0 GROUP BY CITYalias0.STATE_NAME ) AS DERIVED_TABLEalias0 WHERE DERIVED_TABLEalias0.DERIVED_FIELDalias0 = ( SELECT MAX( DERIVED_TABLEalias1.DERIVED_FIELDalias1 ) FROM ( SELECT COUNT( 1 ) AS DERIVED_FIELDalias1 FROM CITY AS CITYalias1 GROUP BY CITYalias1.STATE_NAME ) AS DERIVED_TABLEalias1 ) ) ;\n252 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"maine\" ; \n SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME IN ( SELECT DERIVED_TABLEalias0.STATE_NAME FROM ( SELECT CITYalias0.STATE_NAME , COUNT( 1 ) AS DERIVED_FIELDalias0 FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 GROUP BY CITYalias0.STATE_NAME ) AS DERIVED_TABLEalias0 WHERE DERIVED_TABLEalias0.DERIVED_FIELDalias0 = ( SELECT MAX( DERIVED_TABLEalias1.DERIVED_FIELDalias1 ) FROM ( SELECT COUNT( 1 ) AS DERIVED_FIELDalias1 FROM CITY AS CITYalias1 WHERE CITYalias1.POPULATION > 150000 GROUP BY CITYalias1.STATE_NAME ) AS DERIVED_TABLEalias1 ) ) ;\n253 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ) ; \n SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ) ;\n254 SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ) ; \n SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ) ;\n255 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"at least one major river\" ; \n SELECT DISTINCT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH > 750 ;\n256 SELECT MOUNTAINalias0.MOUNTAIN_NAME FROM MOUNTAIN AS MOUNTAINalias0 WHERE MOUNTAINalias0.MOUNTAIN_ALTITUDE = ( SELECT MAX( MOUNTAINalias1.MOUNTAIN_ALTITUDE ) FROM MOUNTAIN AS MOUNTAINalias1 ) ; \n SELECT MOUNTAINalias0.STATE_NAME FROM MOUNTAIN AS MOUNTAINalias0 ;\n257 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"the united states\" ; \n SELECT MOUNTAINalias0.STATE_NAME FROM MOUNTAIN AS MOUNTAINalias0 WHERE MOUNTAINalias0.MOUNTAIN_ALTITUDE = ( SELECT MAX( MOUNTAINalias1.MOUNTAIN_ALTITUDE ) FROM MOUNTAIN AS MOUNTAINalias1 ) ;\n258 SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = \"small-city-alaska\" ) AND CITYalias0.STATE_NAME = \"small-city-alaska\" ; \n SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 ) ;\n259 SELECT DENSITYalias0.DENSITY FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MAX( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT STATEalias0.DENSITY FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME IN ( SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 ) ) ;\n260 SELECT HIGHLOWalias0.HIGHEST_POINT FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.STATE_NAME = \"alaska\" ; \n SELECT MOUNTAINalias0.MOUNTAIN_NAME FROM MOUNTAIN AS MOUNTAINalias0 WHERE MOUNTAINalias0.MOUNTAIN_ALTITUDE = ( SELECT MAX( MOUNTAINalias1.MOUNTAIN_ALTITUDE ) FROM MOUNTAIN AS MOUNTAINalias1 WHERE MOUNTAINalias1.STATE_NAME <> \"alaska\" ) ;\n261 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"texas\" ; \n SELECT DISTINCT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME NOT IN ( SELECT RIVERalias1.RIVER_NAME FROM RIVER AS RIVERalias1 WHERE RIVERalias1.TRAVERSE = \"texas\" ) ;\n262 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"usa\" ; \n SELECT DISTINCT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.COUNTRY_NAME <> \"usa\" ;\n263 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"austin\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME IN ( SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.CAPITAL = \"austin\" ) ) ;\n264 SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = \"ranches\" ; \n SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE IN ( SELECT DERIVED_TABLEalias0.STATE_NAME FROM ( SELECT CITYalias0.STATE_NAME , COUNT( 1 ) AS DERIVED_FIELDalias0 FROM CITY AS CITYalias0 GROUP BY CITYalias0.STATE_NAME ) AS DERIVED_TABLEalias0 WHERE DERIVED_TABLEalias0.DERIVED_FIELDalias0 = ( SELECT MIN( DERIVED_TABLEalias1.DERIVED_FIELDalias1 ) FROM ( SELECT COUNT( 1 ) AS DERIVED_FIELDalias1 FROM CITY AS CITYalias1 GROUP BY CITYalias1.STATE_NAME ) AS DERIVED_TABLEalias1 ) ) ;\n265 SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.CITY_NAME = \"maine\" ) AND CITYalias0.STATE_NAME = \"maine\" ; \n SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 , STATE AS STATEalias0 WHERE STATEalias0.CAPITAL = CITYalias1.CITY_NAME ) ;\n269 SELECT HIGHLOWalias0.STATE_NAME FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.LOWEST_ELEVATION = ( SELECT MIN( HIGHLOWalias1.LOWEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 ) ; \n SELECT HIGHLOWalias0.STATE_NAME FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.LOWEST_ELEVATION = ( SELECT MIN( HIGHLOWalias1.LOWEST_ELEVATION ) FROM HIGHLOW AS HIGHLOWalias1 WHERE HIGHLOWalias1.STATE_NAME IN ( SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = \"idaho\" ) ) AND HIGHLOWalias0.STATE_NAME IN ( SELECT BORDER_INFOalias1.BORDER FROM BORDER_INFO AS BORDER_INFOalias1 WHERE BORDER_INFOalias1.STATE_NAME = \"idaho\" ) ;\n270 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"rivers\" ) AND RIVERalias0.RIVER_NAME = \"rivers\" ; \n SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH > 750 GROUP BY RIVERalias0.TRAVERSE ORDER BY COUNT( RIVERalias0.RIVER_NAME ) DESC LIMIT 1 ;\n271 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"tribal\" ; \n SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH > 750 GROUP BY RIVERalias0.TRAVERSE ORDER BY COUNT( RIVERalias0.RIVER_NAME ) DESC LIMIT 1 ;\n272 SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MIN( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.CITY_NAME = \"maine\" ) AND CITYalias0.STATE_NAME = \"maine\" ; \n SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 GROUP BY CITYalias0.STATE_NAME ORDER BY AVG ( CITYalias0.POPULATION ) LIMIT 1 ;\n273 SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = ( SELECT MIN( STATEalias1.AREA ) FROM STATE AS STATEalias1 ) ; \n SELECT MOUNTAINalias0.STATE_NAME FROM MOUNTAIN AS MOUNTAINalias0 WHERE MOUNTAINalias0.MOUNTAIN_NAME = \"mckinley\" ;\n274 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = \"rivers\" ; \n SELECT DISTINCT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 ;\n275 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"rivers\" ) AND RIVERalias0.RIVER_NAME = \"rivers\" ; \n SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 GROUP BY RIVERalias0.TRAVERSE ORDER BY COUNT( RIVERalias0.RIVER_NAME ) DESC LIMIT 1 ;\n276 SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.LENGTH = ( SELECT MAX( RIVERalias1.LENGTH ) FROM RIVER AS RIVERalias1 WHERE RIVERalias1.RIVER_NAME = \"rivers\" ) AND RIVERalias0.RIVER_NAME = \"rivers\" ; \n SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 GROUP BY RIVERalias0.TRAVERSE ORDER BY COUNT( RIVERalias0.RIVER_NAME ) DESC LIMIT 1 ;\n146\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"We can also examine the exact match accuracy (i.e., requiring the predicted SQL string to exactly match the gold answer) rather than the execution accuracy (just checking whether the output of executing the SQL against the database is the same).","metadata":{}},{"cell_type":"code","source":"def check_exact_match_accuracy(predictions, data):\n    assert len(predictions) == len(data)\n    correct = 0\n    for p, d in zip(predictions, data):\n        if p == d['sql']:\n            correct += 1\n    return correct / len(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:24:25.766495Z","iopub.execute_input":"2025-01-15T08:24:25.766800Z","iopub.status.idle":"2025-01-15T08:24:25.771237Z","shell.execute_reply.started":"2025-01-15T08:24:25.766776Z","shell.execute_reply":"2025-01-15T08:24:25.770347Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print('finetuned exact match acc:', check_exact_match_accuracy(finetuned_predictions, splits['test']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:24:30.582466Z","iopub.execute_input":"2025-01-15T08:24:30.582767Z","iopub.status.idle":"2025-01-15T08:24:30.587779Z","shell.execute_reply.started":"2025-01-15T08:24:30.582744Z","shell.execute_reply":"2025-01-15T08:24:30.586945Z"}},"outputs":[{"name":"stdout","text":"finetuned exact match acc: 0.4729241877256318\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"The exact match accuracy will likely be close to the execution accuracy, but not exactly the same. What are some potential pros and cons of each metric? Discuss in your report.","metadata":{}},{"cell_type":"markdown","source":"Unload your finetuned model so that you don't run out of GPU memory later.","metadata":{}},{"cell_type":"code","source":"del finetuned_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:24:44.697446Z","iopub.execute_input":"2025-01-15T08:24:44.697773Z","iopub.status.idle":"2025-01-15T08:24:44.703079Z","shell.execute_reply.started":"2025-01-15T08:24:44.697746Z","shell.execute_reply":"2025-01-15T08:24:44.702300Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Few-Shot Prompting","metadata":{}},{"cell_type":"markdown","source":"For the final part of this project, you will explore few-shot prompting, i.e., simply prompting the pretrained language model out-of-the-box using a small number of examples rather than finetuning.\n\nFirst, let's try just selecting 4 examples completely at random from the training set. Rewrite your `predict_greedy` function to change the prompt:","metadata":{}},{"cell_type":"code","source":"import random\n\n\nfew_shot_prompt = \"Question: {question0}\\n\\nSQL: {sql0}\\n\\n\\n\\n\" + \\\n    \"Question: {question1}\\n\\nSQL: {sql1}\\n\\n\\n\\n\" + \\\n    \"Question: {question2}\\n\\nSQL: {sql2}\\n\\n\\n\\n\" + \\\n    \"Question: {question3}\\n\\nSQL: {sql3}\\n\\n\\n\\n\" + \\\n    \"Question: {question}\\n\\nSQL:\"\n\n\ndef select_random_examples(question, few_shot_data, num_examples=4):\n    \"\"\"\n    Return a list containing 4 of the elements of few_shot_data, selected randomly\n    \"\"\"\n    return random.sample(few_shot_data, num_examples)\n\n\n@torch.no_grad()\ndef predict_greedy_fewshot(model, data, few_shot_data, max_new_tokens=128, example_selection_method=select_random_examples):\n    \"\"\"\n    Return the model's greedy text-to-sql predictions on the given data split.\n    The maximum number of new tokens generated (NOT including tokens in the prompt) should be equal to max_new_tokens.\n    The four examples with their SQL outputs should go in {question1}, {sql1}, {question2}, {sql2}, etc. in the few_shot_prompt. \n    The final {question} is the question that we're currently evaluating on.\n    \"\"\"\n    questions = [d['question'] for d in data]\n    predicted_sqls = []\n    prompts = []\n    for question in questions:\n        few_shot_examples = example_selection_method(question, few_shot_data, num_examples=4)\n        prompts.append(few_shot_prompt.format(\n            question0=few_shot_examples[0]['question'],\n            sql0=few_shot_examples[0]['sql'],\n            question1=few_shot_examples[1]['question'],\n            sql1=few_shot_examples[1]['sql'],\n            question2=few_shot_examples[2]['question'],\n            sql2=few_shot_examples[2]['sql'],\n            question3=few_shot_examples[3]['question'],\n            sql3=few_shot_examples[3]['sql'],\n            question=question\n        ))\n    # Your code here; should be fairly similar to your previous predict_greedy code.\n    # Hint: if you batch, we recommend batch size 8-16.\n    batch_size=16\n    tokenizer.padding_side='left'\n    tmp=[]\n    for i in range(0, len(data), batch_size):\n        batch_prompts = prompts[i:i + batch_size]\n        out_all = tokenizer.batch_encode_plus(batch_prompts, return_tensors=\"pt\", padding=True)\n        inputs = out_all.input_ids.cuda()\n        attention_mask = out_all.attention_mask.cuda()\n        output = model.generate(inputs, max_length=max_new_tokens, pad_token_id=tokenizer.pad_token_id, attention_mask=attention_mask, num_return_sequences=1, temperature=0,do_sample=False)\n        generated_sqls = tokenizer.batch_decode(output, skip_special_tokens=True, max_length=max_new_tokens)\n        for generated_sql in generated_sqls:\n            generated_sql_string = generated_sql.split(\"\\n\")\n            sql_strings = [s for s in generated_sql_string if s.startswith('SQL: ')]\n            tmp.append(sql_strings[4])\n\n\n    predicted_sqls = [sql.replace(\"SQL: \", \"\") for sql in tmp]\n    return predicted_sqls # list of strings containing SQL predictions for each question in the data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:26:09.214438Z","iopub.execute_input":"2025-01-15T08:26:09.214770Z","iopub.status.idle":"2025-01-15T08:26:09.222847Z","shell.execute_reply.started":"2025-01-15T08:26:09.214743Z","shell.execute_reply":"2025-01-15T08:26:09.221945Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Reload the gpt2 model if you need to\ngpt2_model = AutoModelForCausalLM.from_pretrained('gpt2-medium').to(device) # Your code here","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:26:21.412709Z","iopub.execute_input":"2025-01-15T08:26:21.413004Z","iopub.status.idle":"2025-01-15T08:26:22.553119Z","shell.execute_reply.started":"2025-01-15T08:26:21.412981Z","shell.execute_reply":"2025-01-15T08:26:22.552276Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# This call can take a few minutes even if you batch; you can debug on a subset of the dev set as needed.\npredictions = predict_greedy_fewshot(gpt2_model, splits['test'], splits['train'])\nprint('4-shot prompting with random examples, execution acc:', check_execution_accuracy(predictions, splits['test']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:26:29.859366Z","iopub.execute_input":"2025-01-15T08:26:29.859694Z","iopub.status.idle":"2025-01-15T08:26:29.907001Z","shell.execute_reply.started":"2025-01-15T08:26:29.859665Z","shell.execute_reply":"2025-01-15T08:26:29.905834Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-40edb2bb3785>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This call can take a few minutes even if you batch; you can debug on a subset of the dev set as needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_greedy_fewshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt2_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'4-shot prompting with random examples, execution acc:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_execution_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-0805b64cbedf>\u001b[0m in \u001b[0;36mpredict_greedy_fewshot\u001b[0;34m(model, data, few_shot_data, max_new_tokens, example_selection_method)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mgenerated_sqls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgenerated_sql\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_sqls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2103\u001b[0m             \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_logits_to_keep\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_generated_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_default_max_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m         \u001b[0;31m# 7. Prepare the cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids_length\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0minput_ids_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_input_ids\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1412\u001b[0m                 \u001b[0;34mf\"Input length of {input_ids_string} is {input_ids_length}, but `max_length` is set to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                 \u001b[0;34mf\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 468, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."],"ename":"ValueError","evalue":"Input length of input_ids is 468, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.","output_type":"error"}],"execution_count":39},{"cell_type":"markdown","source":"You will probably observe between 0-5% accuracy. Random example selection doesn't work very well on this dataset. \n\nHowever, what if we select examples by picking the examples from the training set whose questions are most similar to our current question? To do this, load a pretrained sentence encoder, which takes a sentence as input and outputs a fixed-length vector encoding semantic information about that sentence. First compute the vectors associated with all the training set questions, and then select examples from the training set based on which question vectors have the largest dot products with the vector for your current question. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sentence_transformers import SentenceTransformer\n\n# Your code here; load the sentence encoder (see https://www.sbert.net/ for documentation). A good choice of model is \"all-MiniLM-L6-v2\"\nsentence_encoder = SentenceTransformer('all-MiniLM-L6-v2') \n\ndef compute_question_encodings(data):\n    \"\"\"\n    For each example in the data, add a field called 'question_encoding' to the example, which is the vector encoding of the question.\n    \"\"\"\n    # Your code here\n    vectors = sentence_encoder.encode([d['question'] for d in data],show_progress_bar=False)\n    for example, vector in zip(data, vectors):\n        example['question_encoding'] = np.array(vector)\n\n\ndef select_similar_examples(question, few_shot_data, num_examples=4):\n    \"\"\"\n    Return a list containing 4 of the elements of few_shot_data, selected with questions most semantically similar to the given question. \n    The most similar question should be the LAST element of the list, second most similar should be the second to last element, etc.\n    The reason is that in the few-shot prompt, **you want the best example to be the most recent one.**\n\n    To rank by semantic similarity, first compute the vector for the current question, then compute its dot product with\n    all training set vectors (hint: you may want to vectorize this computation using numpy). Then sort by dot product.\n\n    You should take advantage of the 'question_encoding' field that you added to each example in compute_question_encodings.\n    \"\"\"\n    # Your code here\n    # hint: when you call .encode with your sentence encoder, use show_progress_bar=False to avoid tons of printouts\n    current_question_encoding = sentence_encoder.encode([question], show_progress_bar=False)[0]\n    similarities = np.dot(current_question_encoding, np.array([example['question_encoding'] for example in few_shot_data]).T)\n    sorted_indices = np.argsort(similarities)[::-1]\n    selected_examples = [few_shot_data[i] for i in sorted_indices[:num_examples]]\n    \n    return selected_examples\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# first precompute all the vectors for the training set\ncompute_question_encodings(splits['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# this call will again take a few minutes, even if you batched; feel free to debug on smaller sets of dev\npredictions = predict_greedy_fewshot(gpt2_model, splits['test'], splits['train'], example_selection_method=select_similar_examples)\nprint('4-shot prompting with similar examples, execution acc:', check_execution_accuracy(predictions, splits['test']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You should now achieve about 34% accuracy. The autograder will check that you get >30%.","metadata":{}},{"cell_type":"markdown","source":"Save your predictions.","metadata":{}},{"cell_type":"code","source":"save_predictions(predictions, 'similar4shot_predictions.txt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once again, inspect some of your predictions (from prompting with similar examples) compared to the correct outputs, and describe some common types of errors in your report. Are there any differences compared to the finetuned model, or are the types of errors pretty similar? You can also check the exact match accuracy again.","metadata":{}},{"cell_type":"markdown","source":"Finally, do some open-ended exploration to try to improve your performance on this dataset as much as possible (whether for finetuning or prompting). No hard requirement on how much to improve (or to improve at all), but please discuss the ideas you tried + how effective they were in your report. (Be careful with the GPU memory if you're using Kaggle, though- we're already nearly capping out the GPU memory in a few places with the current settings.)\n\nA non-exhaustive list of possible ideas:\n* Use a different similarity metric for selecting examples in few-shot prompting\n* Use more examples in few-shot prompting\n* Load a different base model than GPT2-Medium, or look into calling the OpenAI API\n* Tune the hyperparameters used for finetuning\n* Try to combine few-shot prompting with finetuning","metadata":{}},{"cell_type":"markdown","source":"Your final submission should include the following files:\n\n* hw4.ipynb (this file; please rename to match)\n* finetuned_predictions.txt\n* similar4shot_predictions.txt\n* report.pdf","metadata":{}}]}