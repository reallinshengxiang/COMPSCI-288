{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 5: Multimodal Models\n\nIn this project, you will explore multimodal tasks that interface language with vision, such as image retrieval, caption retrieval, and image captioning. We will work with [Concadia](https://arxiv.org/abs/2104.08376), an image-text dataset scraped from Wikipedia focused on the relationship between images and the textual context. First, you will use CLIP for image retrieval and caption retrieval. Then, we will explore how implementing a Rational Speech Acts inference procedure can improve retrieval performance. Finally, we will generate our own captions for these images.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Setup\n\nFirst, we will install the required dependencies and download the dataset.\n\nIn Kaggle, you may want to turn on file persistence (Right Sidebar > Session Options > Persistence > Files only) so you don't have to re-download the dataset every time you restart this notebook.","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:37:43.558929Z","iopub.execute_input":"2023-11-07T00:37:43.55923Z","iopub.status.idle":"2023-11-07T00:37:43.587659Z","shell.execute_reply.started":"2023-11-07T00:37:43.559204Z","shell.execute_reply":"2023-11-07T00:37:43.586766Z"}}},{"cell_type":"code","source":"%%capture\n!pip install transformers\n!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:29:34.849058Z","iopub.execute_input":"2023-11-11T01:29:34.849528Z","iopub.status.idle":"2023-11-11T01:29:40.693962Z","shell.execute_reply.started":"2023-11-11T01:29:34.84948Z","shell.execute_reply":"2023-11-11T01:29:40.692579Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n# Download dataset\n# resized.zip\n!gdown --id 1gDhVlOwcGcwBT5LWYwgn9xEElGlKVpFb\n!unzip /kaggle/working/resized.zip\n# wiki_split.json\n!gdown --id 1kiTSiqk7y7JdHssXjoLwcOomC7lhb5k8","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer, AutoProcessor, AutoModelForCausalLM\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:30:32.78202Z","iopub.status.idle":"2023-11-11T01:30:32.78242Z","shell.execute_reply.started":"2023-11-11T01:30:32.782235Z","shell.execute_reply":"2023-11-11T01:30:32.782254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Before proceeding, let's verify that we're connected to a GPU runtime and that torch can detect the GPU. We'll define a variable device here to use throughout the code so that we can easily change to run on CPU for debugging.","metadata":{}},{"cell_type":"code","source":"assert torch.cuda.is_available()\ndevice = torch.device(\"cuda\")\nprint(\"Using device:\", device)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:29:59.544547Z","iopub.execute_input":"2023-11-11T01:29:59.545Z","iopub.status.idle":"2023-11-11T01:29:59.620708Z","shell.execute_reply.started":"2023-11-11T01:29:59.544957Z","shell.execute_reply":"2023-11-11T01:29:59.619404Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Setup\n\nFor this project, let's load up the validation split of the dataset and inspect a sample.\n\nConcadia consists of images paired with _captions_, which provide additional context, and _descriptions_, which are intended to replace the image (e.g. for alt-text). We'll be comparing captions and descriptions for various multimodal tasks, so feel free to explore what those instances look like. You may also want to refer to the paper for context.","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/working/wiki_split.json\") as f:\n    data = json.load(f)[\"images\"]\nval = [d for d in data if d[\"split\"] == \"val\"]\nprint(\"Number of validation examples:\", len(val))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:29:59.622276Z","iopub.execute_input":"2023-11-11T01:29:59.622714Z","iopub.status.idle":"2023-11-11T01:30:07.117181Z","shell.execute_reply.started":"2023-11-11T01:29:59.62267Z","shell.execute_reply":"2023-11-11T01:30:07.115859Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect a data sample\nimg = Image.open(f\"/kaggle/working/resized/{val[0]['filename']}\")\ndisplay(img)\nprint(\"Caption:\", val[0][\"caption\"][\"raw\"])\nprint(\"Description:\", val[0][\"description\"][\"raw\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:30:07.118831Z","iopub.execute_input":"2023-11-11T01:30:07.119171Z","iopub.status.idle":"2023-11-11T01:30:07.170548Z","shell.execute_reply.started":"2023-11-11T01:30:07.119142Z","shell.execute_reply":"2023-11-11T01:30:07.169393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 1: Image & Text Retrieval\n","metadata":{}},{"cell_type":"markdown","source":"First, we'll retrieve images from the dataset from captions and descriptions, and evaluate the relative performance of retrieving with captions vs. descriptions for a subset of the validation set.","metadata":{}},{"cell_type":"code","source":"model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Evaluate on a subset of the validation set\nMAX_LEN = 70\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\ndata = [d for d in val if len(tokenizer(d[\"description\"][\"raw\"])[\"input_ids\"]) < MAX_LEN\n        and len(tokenizer(d[\"caption\"][\"raw\"])[\"input_ids\"]) < MAX_LEN]\ndata = data[:1000]","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:30:39.8908Z","iopub.execute_input":"2023-11-11T01:30:39.891255Z","iopub.status.idle":"2023-11-11T01:30:54.047117Z","shell.execute_reply.started":"2023-11-11T01:30:39.89122Z","shell.execute_reply":"2023-11-11T01:30:54.045734Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rank_images(queries, images):\n    \"\"\"Ranks images for a batch of text queries by their CLIP similarity score.\n    Args:\n        queries: a list with batch of N text queries\n        images: a list with a batch of N images\n    Returns:\n         A NxN array with image indices sorted by retrieval score for each query.\n    \"\"\"\n    # YOUR CODE HERE\n    ##","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:30:54.049244Z","iopub.execute_input":"2023-11-11T01:30:54.049597Z","iopub.status.idle":"2023-11-11T01:30:54.058649Z","shell.execute_reply.started":"2023-11-11T01:30:54.049565Z","shell.execute_reply":"2023-11-11T01:30:54.057523Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You will implement several retrieval metrics:\n* Mean reciprocal rank: the average 1/rank of the true image for a query\n* Top-1 accuracy: the % of instances where the true image is the top result for a query\n* Top-k accuracy (k=10): the % of instances where the true image is in the top k results for a query","metadata":{}},{"cell_type":"code","source":"def mrr(batch_outputs):\n    # YOUR CODE HERE\n    ###\n\ndef top1_accuracy(batch_outputs):\n    # YOUR CODE HERE\n    ### \n    \ndef topk_accuracy(batch_outputs, k=10):\n    # YOUR CODE HERE\n    ###","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:30:54.05978Z","iopub.execute_input":"2023-11-11T01:30:54.060085Z","iopub.status.idle":"2023-11-11T01:30:54.076742Z","shell.execute_reply.started":"2023-11-11T01:30:54.060058Z","shell.execute_reply":"2023-11-11T01:30:54.075645Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we'll compute each of these metrics for retrieving images based on captions vs. descriptions.\n\nYou should be getting >.60 MRR, >50% top-1 accuracy and >80% top-10 accuracy for both.","metadata":{"execution":{"iopub.status.busy":"2023-11-08T21:16:40.972276Z","iopub.execute_input":"2023-11-08T21:16:40.97271Z","iopub.status.idle":"2023-11-08T21:16:40.978515Z","shell.execute_reply.started":"2023-11-08T21:16:40.972679Z","shell.execute_reply":"2023-11-08T21:16:40.977417Z"}}},{"cell_type":"code","source":"captions = [ex[\"caption\"][\"raw\"] for ex in data]\ndescs = [ex[\"description\"][\"raw\"] for ex in data]\nimages = [Image.open(f\"/kaggle/working/resized/{ex['filename']}\") for ex in data]","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:30:54.079185Z","iopub.execute_input":"2023-11-11T01:30:54.079571Z","iopub.status.idle":"2023-11-11T01:30:54.244423Z","shell.execute_reply.started":"2023-11-11T01:30:54.07954Z","shell.execute_reply":"2023-11-11T01:30:54.243466Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_outputs_cap = rank_images(captions, images)\nprint(\"Caption-based retrieval:\")\nprint(\"MRR: \", mrr(batch_outputs_cap))\nprint(\"Top-1 acc: \", top1_accuracy(batch_outputs_cap))\nprint(\"Top-10 acc: \", topk_accuracy(batch_outputs_cap))\nnp.save(\"image_retrieval_caption.npy\", batch_outputs_cap)\n\nbatch_outputs_desc = rank_images(descs, images)\nprint(\"Descriptions-based retrieval:\")\nprint(\"MRR: \", mrr(batch_outputs_desc))\nprint(\"Top-1 acc: \", top1_accuracy(batch_outputs_desc))\nprint(\"Top-10 acc: \", topk_accuracy(batch_outputs_desc))\nnp.save(\"image_retrieval_desc.npy\", batch_outputs_desc)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:30:54.245604Z","iopub.execute_input":"2023-11-11T01:30:54.245896Z","iopub.status.idle":"2023-11-11T01:31:35.427868Z","shell.execute_reply.started":"2023-11-11T01:30:54.245871Z","shell.execute_reply":"2023-11-11T01:31:35.42652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we'll look at the inverse problem: for a given image, does the corresponding caption or description in the dataset score higher?","metadata":{}},{"cell_type":"code","source":"def caption_better(captions, descriptions, images):\n    \"\"\"For a batch of paired (caption, description, image), returns True for each batch element\n    if the caption scores higher for the image than the description.\n    Args:\n        captions: a list with batch of N text captions\n        descriptions:  list with batch of N text descriptions\n        images: a list with a batch of N images\n    Returns:\n         A list of N booleans, True if the caption scores higher for the image\n    \"\"\"\n    # YOUR CODE HERE\n    ##\n    \nprint(\"% of examples where caption scores more highly:\", np.mean(caption_better(captions, descs, images)))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:31:35.429157Z","iopub.execute_input":"2023-11-11T01:31:35.429531Z","iopub.status.idle":"2023-11-11T01:32:15.663729Z","shell.execute_reply.started":"2023-11-11T01:31:35.429501Z","shell.execute_reply":"2023-11-11T01:32:15.662593Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You should see >50% of examples where the caption scores more highly.","metadata":{}},{"cell_type":"markdown","source":"### Qualitative Analysis Part 1: Image & Text Retrieval\n\nIn the report, provide the following analysis:\n* Investigate a couple of examples where image retrieval fails, and systematic model issues you observe (if any).\n* Are there any differences between the failure modes of retrieving based on captions and retrieving based on descriptions (e.g. for the same example)? You may also look at some examples of caption and description text and comment on where you think their performance could differ in general.\n* Discuss how the retrieval metrics affect analysis. All of these metrics are commonly reported in information retrieval — how do the metrics (and your qualitiative analysis from the last two steps) provide different perspectives on model performance? \n* Investigate a couple of examples where the paired caption scores more highly than the paired description for an image. Are there any patterns you observe that determine whether the caption or the description scores more highly?","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE\n###","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-11T01:32:15.665055Z","iopub.execute_input":"2023-11-11T01:32:15.665357Z","iopub.status.idle":"2023-11-11T01:32:15.973826Z","shell.execute_reply.started":"2023-11-11T01:32:15.66533Z","shell.execute_reply":"2023-11-11T01:32:15.97277Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 2: Rational Speech Acts (RSA)\n\nNow we will implement RSA on top of our simple image retrieval system. Recall that the idea of RSA is to model how likely an utterance is _in context_. Even if a caption scores the highest for a given image based on raw similarity score, it may no longer be the best text in the context of other, alternative captions for the image. We will implement this idea and investigate how it changes retrieval performance.","metadata":{}},{"cell_type":"markdown","source":"We'll work with the examples from Part 1 where the true image is not in the top 10 retrieval results for a given caption. You should double-check your retrieval performance matches the expected numbers before you proceed; otherwise, you'll be working with different examples here. ","metadata":{}},{"cell_type":"code","source":"incorrect = [b[:10] for i, b in enumerate(batch_outputs_cap) if i not in b[:10]]\nincorrect_idxs = [i for i, b in enumerate(batch_outputs_cap) if i not in b[:10]]\nassert len(incorrect) == 92, \"Your retrieval system results do not match ours.\"","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:32:15.975217Z","iopub.execute_input":"2023-11-11T01:32:15.975654Z","iopub.status.idle":"2023-11-11T01:32:15.99721Z","shell.execute_reply.started":"2023-11-11T01:32:15.975616Z","shell.execute_reply":"2023-11-11T01:32:15.995891Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For each of the captions with an incorrect retrieval result, we'll get the top 10 retrieved images and the true captions for each of those 10 images. The alternative captions will form the context for RSA.\n\nLet's implement the RSA functions that we'll run on these contexts.","metadata":{"execution":{"iopub.status.busy":"2023-11-08T23:49:09.25954Z","iopub.execute_input":"2023-11-08T23:49:09.260225Z","iopub.status.idle":"2023-11-08T23:49:09.266529Z","shell.execute_reply.started":"2023-11-08T23:49:09.260191Z","shell.execute_reply":"2023-11-08T23:49:09.265609Z"}}},{"cell_type":"markdown","source":"First, implement a **literal listener** which retrieves the top images for a given caption without modeling how likely a speaker is to generate those captions.\n    \nThis should look similar to your rank_images retrieval function from Part 1, except the function should now return softmax-normalized probabilities (which will then be used by the pragmatic speaker).","metadata":{}},{"cell_type":"code","source":"def literal_listener(captions, images):\n    \"\"\"\n    Args:\n        captions: list of N captions\n        images: list of N images\n    Returns:\n        a NxN array of normalized conditional probabilities,\n        where A[i][j] = p(image j | caption i)\n    \"\"\"\n    # YOUR CODE HERE\n    ###\n    assert np.allclose(listener_probs.sum(1), 1)\n    return listener_probs","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:32:15.99861Z","iopub.execute_input":"2023-11-11T01:32:15.998993Z","iopub.status.idle":"2023-11-11T01:32:16.011824Z","shell.execute_reply.started":"2023-11-11T01:32:15.998954Z","shell.execute_reply":"2023-11-11T01:32:16.010807Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we'll implement a **pragmatic speaker**. For a given image, the pragmatic speaker is more likely to select utterances that elicit the right inference in the listener (i.e. the more likely the listener is to choose the right image based on the caption, the more likely the speaker is to use that caption).\n\nThe pragmatic speaker should call the literal_listener within the function. Normalize your speaker probabilities by dividing by the sum (rather than using softmax), matching the implementation in lecture.","metadata":{}},{"cell_type":"code","source":"def pragmatic_speaker(captions, images):\n    \"\"\"\n    Args:\n        captions: list of N captions\n        images: list of N images\n    Returns:\n        a NxN array of normalized conditional probabilities,\n        where A[i][j] = p(caption i | image j)\n    \"\"\"\n    # YOUR CODE HERE\n    ###\n    assert np.allclose(speaker_probs.sum(0), 1)\n    return speaker_probs","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:32:16.015677Z","iopub.execute_input":"2023-11-11T01:32:16.016057Z","iopub.status.idle":"2023-11-11T01:32:16.022931Z","shell.execute_reply.started":"2023-11-11T01:32:16.016026Z","shell.execute_reply":"2023-11-11T01:32:16.021942Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally, implement a **pragmatic listener**, which models how a pragmatic speaker would select utterances. Similarly, you should call pragmatic_speaker within the function and use sum-normalization instead of softmax.","metadata":{}},{"cell_type":"code","source":"def pragmatic_listener(captions, images):\n    \"\"\"\n    Args:\n        captions: list of N captions\n        images: list of N images\n    Returns:\n        a NxN array of normalized conditional probabilities,\n        where A[i][j] = p(image j | caption i)\n    \"\"\"\n    # YOUR CODE HERE\n    ###\n    assert np.allclose(listener_probs.sum(1), 1)\n    return listener_probs","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:32:16.0242Z","iopub.execute_input":"2023-11-11T01:32:16.024573Z","iopub.status.idle":"2023-11-11T01:32:16.035138Z","shell.execute_reply.started":"2023-11-11T01:32:16.024544Z","shell.execute_reply":"2023-11-11T01:32:16.034186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's evaluate the pragmatic listener on each context. You should get >20 examples (out of 92) where pragmatics fixes the error.","metadata":{"execution":{"iopub.status.busy":"2023-11-09T00:12:54.420228Z","iopub.execute_input":"2023-11-09T00:12:54.420618Z","iopub.status.idle":"2023-11-09T00:12:54.426846Z","shell.execute_reply.started":"2023-11-09T00:12:54.420564Z","shell.execute_reply":"2023-11-09T00:12:54.425268Z"}}},{"cell_type":"code","source":"improved = 0\npredictions = []\n\nfor true_idx, topk_idxs in zip(incorrect_idxs, incorrect):\n    img_context = [images[idx] for idx in [true_idx] + topk_idxs.tolist()]\n    caption_context = [captions[idx] for idx in [true_idx] + topk_idxs.tolist()]\n    # Call your pragmatic listener and increment `improved` if the top image candidate is now correct\n    # YOUR CODE HERE\n    ###\n    # Save prediction outputs\n    predictions.append(pragmatic_probs)\n        \nprint(f\"# examples where pragmatics fixes the error: {improved} / {len(incorrect)}\")\nnp.save(\"pragmatic_preds.npy\", np.array(predictions))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:32:16.03626Z","iopub.execute_input":"2023-11-11T01:32:16.036615Z","iopub.status.idle":"2023-11-11T01:32:34.792953Z","shell.execute_reply.started":"2023-11-11T01:32:16.036587Z","shell.execute_reply":"2023-11-11T01:32:34.791774Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Qualitative Analysis Part 2: RSA\n\nIn your report:\n* Report the # of examples that were previously incorrect that are now correct (the correct image is now the top candidate).\n* Investigate some qualitative examples to get an intuition of why RSA fixes these issues and discuss one of these examples (e.g. compare the literal listener probabilities to the pragmatic listener probabilities and think about why they changed). ","metadata":{"execution":{"iopub.status.busy":"2023-11-09T01:16:09.281525Z","iopub.execute_input":"2023-11-09T01:16:09.282388Z","iopub.status.idle":"2023-11-09T01:16:09.28842Z","shell.execute_reply.started":"2023-11-09T01:16:09.282353Z","shell.execute_reply":"2023-11-09T01:16:09.287329Z"}}},{"cell_type":"code","source":"# YOUR CODE HERE\n###","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:32:34.79427Z","iopub.execute_input":"2023-11-11T01:32:34.794756Z","iopub.status.idle":"2023-11-11T01:32:35.295937Z","shell.execute_reply.started":"2023-11-11T01:32:34.794713Z","shell.execute_reply":"2023-11-11T01:32:35.294791Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 3: Image Captioning\n\nFinally, we'll explore caption generation using a pretrained model, and how RSA can help us generate more discriminative captions.","metadata":{}},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForConditionalGeneration\n\ncap_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\ncap_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:32:35.29753Z","iopub.execute_input":"2023-11-11T01:32:35.297979Z","iopub.status.idle":"2023-11-11T01:32:50.460924Z","shell.execute_reply.started":"2023-11-11T01:32:35.29794Z","shell.execute_reply":"2023-11-11T01:32:50.459881Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate image captions for a subset of data\npeople = [d for d in val if \" man \" in d[\"description\"][\"raw\"] or \" woman \" in d[\"description\"][\"raw\"]]\npeople = people[:100]\npeople_images = [Image.open(f\"/kaggle/working/resized/{ex['filename']}\") for ex in people]","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:32:50.462296Z","iopub.execute_input":"2023-11-11T01:32:50.462652Z","iopub.status.idle":"2023-11-11T01:32:50.494924Z","shell.execute_reply.started":"2023-11-11T01:32:50.462623Z","shell.execute_reply":"2023-11-11T01:32:50.493711Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We'll compare the image retrieval performance for a literal captioner to that of a pragmatic captioner.\n\nFirst, implement a literal captioner by just decoding the greedy output from the base captioning model.","metadata":{}},{"cell_type":"code","source":"def literal_captioner(image):\n    # YOUR CODE HERE\n    ###","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:32:50.496098Z","iopub.execute_input":"2023-11-11T01:32:50.496466Z","iopub.status.idle":"2023-11-11T01:32:54.202493Z","shell.execute_reply.started":"2023-11-11T01:32:50.49642Z","shell.execute_reply":"2023-11-11T01:32:54.201428Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, implement a pragmatic captioner by sampling candidate captions from a base captioning model, and then using the `pragmatic_speaker` function you implemented earlier to re-rank captions using the other images in the subset of 100 images of people to normalize your speaker probabilities. Use temperature 0.6 and top_p 0.95 for sampling.","metadata":{"execution":{"iopub.status.busy":"2023-11-09T01:44:56.108634Z","iopub.execute_input":"2023-11-09T01:44:56.109394Z","iopub.status.idle":"2023-11-09T01:44:56.117906Z","shell.execute_reply.started":"2023-11-09T01:44:56.109361Z","shell.execute_reply":"2023-11-09T01:44:56.116893Z"}}},{"cell_type":"code","source":"def generate_candidate_captions(image, k=10):\n    \"\"\"Generate k candidate captions for an image.\"\"\"\n    # YOUR CODE HERE\n    ###\n    assert isinstance(candidates, list) and len(candidates) == k and isinstance(candidates[0], str)\n    return candidates\n    \ndef pragmatic_captioner(image, context_images):\n    \"\"\"Generate a caption for the image using a set of context images.\"\"\"\n    # Generate candidate captions\n    # YOUR CODE HERE\n    ###\n    # Rank captions with the pragmatic_speaker and return the best one\n    pragmatic_caption = ... # YOUR CODE HERE\n    return pragmatic_caption","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = []\n\nfor i in range(len(people_images)):\n    literal_caption = literal_captioner(people_images[i])\n    # Call the pragmatic captioner and return the caption after RSA and the candidate captions\n    # Be careful when constructing the context images, which should not include the true image.\n    pragmatic_caption = ... # YOUR CODE HERE\n    results.append((literal_caption, pragmatic_caption))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T02:43:09.515863Z","iopub.execute_input":"2023-11-11T02:43:09.516321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect some results\nfor img, r in zip(people_images[:10], results[:10]):\n    display(img)\n    print(\"Literal caption: \", r[0])\n    print(\"Pragmatic caption: \", r[1])","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, if we were to do image retrieval with our generated captions, let's take a look at how retrieval performance changes if we retrieve based on the greedy output from the captioner vs. the more discriminative captions selected by RSA.\n\nYour top-1 retrieval accuracy with pragmatic captions should be >70%.","metadata":{}},{"cell_type":"code","source":"literal_retrieval_outputs = rank_images([r[0] for r in results], people_images)\nprint(\"Top-1 accuracy (literal):\", top1_accuracy(literal_retrieval_outputs))\npragmatic_retrieval_outputs = rank_images([r[1] for r in results], people_images)\nprint(\"Top-1 accuracy (pragmatic):\", top1_accuracy(pragmatic_retrieval_outputs))\n\nnp.save(\"image_retrieval_pragmatic.npy\", pragmatic_retrieval_outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Qualitative Analysis Part 3: Captioning\n\nIn your report:\n* Look at a few examples of captions generated by the literal and the pragmatic captioner. What are some qualitative trends and why would they come out of RSA inference?","metadata":{}},{"cell_type":"markdown","source":"# Submission\n\nYour final submission should include the following files:\n\n* hw5.ipynb (this file; please rename to match)\n* image_retrieval_caption.npy\n* image_retrieval_desc.npy\n* pragmatic_preds.npy\n* image_retrieval_pragmatic.npy\n* report.pdf (containing qualitative analysis for Parts 1-3)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:30:32.780607Z","iopub.status.idle":"2023-11-11T01:30:32.781023Z","shell.execute_reply.started":"2023-11-11T01:30:32.780835Z","shell.execute_reply":"2023-11-11T01:30:32.780853Z"}}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}